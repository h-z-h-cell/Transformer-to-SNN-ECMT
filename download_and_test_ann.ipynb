{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1a64290-80c9-4d25-8ad8-4c119043d4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python download_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a153f2-7a37-49b5-bbc3-8b1b1f030d06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:343: UserWarning: Overwriting vit_tiny_patch16_224 in registry with model_vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:353: UserWarning: Overwriting vit_tiny_patch16_384 in registry with model_vit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:386: UserWarning: Overwriting vit_small_patch32_224 in registry with model_vit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:398: UserWarning: Overwriting vit_small_patch32_384 in registry with model_vit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:410: UserWarning: Overwriting vit_small_patch16_224 in registry with model_vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:421: UserWarning: Overwriting vit_small_patch16_384 in registry with model_vit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:454: UserWarning: Overwriting vit_base_patch32_224 in registry with model_vit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained: bool = False, **kwargs) -> VisionTransformer:\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:465: UserWarning: Overwriting vit_base_patch32_384 in registry with model_vit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:476: UserWarning: Overwriting vit_base_patch16_224 in registry with model_vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:509: UserWarning: Overwriting vit_base_patch16_384 in registry with model_vit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:520: UserWarning: Overwriting vit_large_patch32_224 in registry with model_vit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:531: UserWarning: Overwriting vit_large_patch32_384 in registry with model_vit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:542: UserWarning: Overwriting vit_large_patch16_224 in registry with model_vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:553: UserWarning: Overwriting vit_large_patch16_384 in registry with model_vit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "Namespace(batch_size=256,\n",
      "data_set='image_folder',\n",
      "device='cuda',\n",
      "distributed=False,\n",
      "eval_data_path='../data/val',\n",
      "imagenet_default_mean_and_std=False,\n",
      "input_size=224,\n",
      "linear_num=8,\n",
      "model='vit_small_patch16_224',\n",
      "model_path='../models/original_model/vit_small_patch16_224.pth',\n",
      "monitor=True,\n",
      "nb_classes=1000,\n",
      "num_workers=10,\n",
      "output_dir='../models',\n",
      "percent=0.99,\n",
      "qkv_num=8,\n",
      "savename='test',\n",
      "seed=0,\n",
      "softmax_num=8,\n",
      "softmax_p=0.0125,\n",
      "test_T=8,\n",
      "test_mode='ann')\n",
      "Number of the class = 1000\n",
      " patchembed dropout sequential layernorm identity linear \n",
      "patch_embed conv2d identity \n",
      "blocks block block block block block block block block block block block block \n",
      "0 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "1 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "2 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "3 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "4 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "5 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "6 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "7 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "8 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "9 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "10 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "11 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "Load ckpt from ../models/original_model/vit_small_patch16_224.pth\n",
      "Load state_dict by model_key = model\n",
      "number of params: 22050664\n",
      "Batch size = 256\n",
      "1 : 85.15625 98.4375\n",
      "* Acc@1 85.156 Acc@5 98.438 loss 1.888\n",
      "Test ANN :  [  0/196]  eta: 0:14:33  loss: 1.8884 (1.8884)  acc1: 85.1562 (85.1562)  acc5: 98.4375 (98.4375)  time: 4.4555  data: 3.2767  max mem: 1402\n",
      "2 : 88.671875 97.65625\n",
      "* Acc@1 86.914 Acc@5 98.047 loss 1.917\n",
      "3 : 93.75 99.609375\n",
      "* Acc@1 89.193 Acc@5 98.568 loss 1.851\n",
      "4 : 92.578125 96.484375\n",
      "* Acc@1 90.039 Acc@5 98.047 loss 1.853\n",
      "5 : 92.578125 98.828125\n",
      "* Acc@1 90.547 Acc@5 98.203 loss 1.832\n",
      "6 : 82.03125 94.921875\n",
      "* Acc@1 89.128 Acc@5 97.656 loss 1.879\n",
      "7 : 69.140625 94.140625\n",
      "* Acc@1 86.272 Acc@5 97.154 loss 1.942\n",
      "8 : 73.4375 96.484375\n",
      "* Acc@1 84.668 Acc@5 97.070 loss 1.971\n",
      "9 : 79.296875 91.796875\n",
      "* Acc@1 84.071 Acc@5 96.484 loss 2.003\n",
      "10 : 76.953125 94.140625\n",
      "* Acc@1 83.359 Acc@5 96.250 loss 2.022\n",
      "11 : 78.125 94.921875\n",
      "* Acc@1 82.884 Acc@5 96.129 loss 2.046\n",
      "Test ANN :  [ 10/196]  eta: 0:01:34  loss: 2.1122 (2.0456)  acc1: 82.0312 (82.8835)  acc5: 96.4844 (96.1293)  time: 0.5086  data: 0.2981  max mem: 1402\n",
      "12 : 73.4375 93.359375\n",
      "* Acc@1 82.096 Acc@5 95.898 loss 2.073\n",
      "13 : 70.703125 91.796875\n",
      "* Acc@1 81.220 Acc@5 95.583 loss 2.101\n",
      "14 : 78.125 95.703125\n",
      "* Acc@1 80.999 Acc@5 95.592 loss 2.106\n",
      "15 : 73.4375 98.046875\n",
      "* Acc@1 80.495 Acc@5 95.755 loss 2.115\n",
      "16 : 78.515625 93.359375\n",
      "* Acc@1 80.371 Acc@5 95.605 loss 2.124\n",
      "17 : 88.671875 98.4375\n",
      "* Acc@1 80.859 Acc@5 95.772 loss 2.111\n",
      "18 : 96.484375 98.4375\n",
      "* Acc@1 81.727 Acc@5 95.920 loss 2.089\n",
      "19 : 93.75 97.265625\n",
      "* Acc@1 82.360 Acc@5 95.991 loss 2.075\n",
      "20 : 91.796875 98.4375\n",
      "* Acc@1 82.832 Acc@5 96.113 loss 2.063\n",
      "21 : 83.59375 93.75\n",
      "* Acc@1 82.868 Acc@5 96.001 loss 2.068\n",
      "Test ANN :  [ 20/196]  eta: 0:00:56  loss: 2.1709 (2.0684)  acc1: 79.2969 (82.8683)  acc5: 95.7031 (96.0007)  time: 0.1134  data: 0.0002  max mem: 1402\n",
      "22 : 87.5 97.265625\n",
      "* Acc@1 83.079 Acc@5 96.058 loss 2.061\n",
      "23 : 89.0625 98.046875\n",
      "* Acc@1 83.339 Acc@5 96.145 loss 2.055\n",
      "24 : 83.59375 96.484375\n",
      "* Acc@1 83.350 Acc@5 96.159 loss 2.057\n",
      "25 : 81.25 93.75\n",
      "* Acc@1 83.266 Acc@5 96.062 loss 2.064\n",
      "26 : 92.1875 98.828125\n",
      "* Acc@1 83.609 Acc@5 96.169 loss 2.053\n",
      "27 : 92.96875 98.4375\n",
      "* Acc@1 83.955 Acc@5 96.253 loss 2.045\n",
      "28 : 91.015625 98.4375\n",
      "* Acc@1 84.208 Acc@5 96.331 loss 2.036\n",
      "29 : 94.140625 98.828125\n",
      "* Acc@1 84.550 Acc@5 96.417 loss 2.023\n",
      "30 : 83.203125 95.703125\n",
      "* Acc@1 84.505 Acc@5 96.393 loss 2.022\n",
      "31 : 80.46875 94.140625\n",
      "* Acc@1 84.375 Acc@5 96.321 loss 2.026\n",
      "Test ANN :  [ 30/196]  eta: 0:00:43  loss: 1.9259 (2.0260)  acc1: 83.5938 (84.3750)  acc5: 97.2656 (96.3206)  time: 0.1240  data: 0.0115  max mem: 1402\n",
      "32 : 73.4375 94.921875\n",
      "* Acc@1 84.033 Acc@5 96.277 loss 2.034\n",
      "33 : 57.421875 91.40625\n",
      "* Acc@1 83.227 Acc@5 96.129 loss 2.054\n",
      "34 : 78.125 96.484375\n",
      "* Acc@1 83.077 Acc@5 96.140 loss 2.060\n",
      "35 : 82.421875 95.703125\n",
      "* Acc@1 83.058 Acc@5 96.127 loss 2.059\n",
      "36 : 75.78125 95.703125\n",
      "* Acc@1 82.856 Acc@5 96.115 loss 2.064\n",
      "37 : 72.265625 93.75\n",
      "* Acc@1 82.570 Acc@5 96.052 loss 2.071\n",
      "38 : 74.609375 91.40625\n",
      "* Acc@1 82.360 Acc@5 95.929 loss 2.077\n",
      "39 : 77.34375 96.875\n",
      "* Acc@1 82.232 Acc@5 95.954 loss 2.079\n",
      "40 : 66.40625 97.265625\n",
      "* Acc@1 81.836 Acc@5 95.986 loss 2.084\n",
      "41 : 80.46875 96.09375\n",
      "* Acc@1 81.803 Acc@5 95.989 loss 2.081\n",
      "Test ANN :  [ 40/196]  eta: 0:00:36  loss: 2.1046 (2.0814)  acc1: 80.4688 (81.8026)  acc5: 96.0938 (95.9889)  time: 0.1371  data: 0.0248  max mem: 1402\n",
      "42 : 77.34375 94.53125\n",
      "* Acc@1 81.696 Acc@5 95.954 loss 2.083\n",
      "43 : 82.03125 98.4375\n",
      "* Acc@1 81.704 Acc@5 96.012 loss 2.079\n",
      "44 : 77.34375 93.75\n",
      "* Acc@1 81.605 Acc@5 95.961 loss 2.084\n",
      "45 : 79.296875 91.40625\n",
      "* Acc@1 81.554 Acc@5 95.859 loss 2.091\n",
      "46 : 74.21875 96.484375\n",
      "* Acc@1 81.394 Acc@5 95.873 loss 2.091\n",
      "47 : 69.921875 96.09375\n",
      "* Acc@1 81.150 Acc@5 95.878 loss 2.095\n",
      "48 : 74.21875 98.4375\n",
      "* Acc@1 81.006 Acc@5 95.931 loss 2.096\n",
      "49 : 70.703125 96.875\n",
      "* Acc@1 80.796 Acc@5 95.950 loss 2.100\n",
      "50 : 93.359375 98.828125\n",
      "* Acc@1 81.047 Acc@5 96.008 loss 2.093\n",
      "51 : 89.0625 96.875\n",
      "* Acc@1 81.204 Acc@5 96.025 loss 2.089\n",
      "Test ANN :  [ 50/196]  eta: 0:00:31  loss: 2.2112 (2.0891)  acc1: 75.7812 (81.2040)  acc5: 96.0938 (96.0248)  time: 0.1423  data: 0.0301  max mem: 1402\n",
      "52 : 76.953125 95.703125\n",
      "* Acc@1 81.122 Acc@5 96.019 loss 2.090\n",
      "53 : 73.4375 94.921875\n",
      "* Acc@1 80.977 Acc@5 95.998 loss 2.092\n",
      "54 : 80.859375 94.53125\n",
      "* Acc@1 80.975 Acc@5 95.971 loss 2.097\n",
      "55 : 75.0 96.875\n",
      "* Acc@1 80.866 Acc@5 95.987 loss 2.101\n",
      "56 : 68.75 94.53125\n",
      "* Acc@1 80.650 Acc@5 95.961 loss 2.104\n",
      "57 : 84.765625 98.4375\n",
      "* Acc@1 80.722 Acc@5 96.005 loss 2.103\n",
      "58 : 93.359375 97.65625\n",
      "* Acc@1 80.940 Acc@5 96.033 loss 2.097\n",
      "59 : 82.03125 95.703125\n",
      "* Acc@1 80.959 Acc@5 96.028 loss 2.099\n",
      "60 : 77.734375 97.265625\n",
      "* Acc@1 80.905 Acc@5 96.048 loss 2.100\n",
      "61 : 79.6875 96.09375\n",
      "* Acc@1 80.885 Acc@5 96.049 loss 2.099\n",
      "Test ANN :  [ 60/196]  eta: 0:00:27  loss: 2.1443 (2.0993)  acc1: 77.3438 (80.8850)  acc5: 96.0938 (96.0489)  time: 0.1488  data: 0.0365  max mem: 1402\n",
      "62 : 80.859375 93.359375\n",
      "* Acc@1 80.885 Acc@5 96.006 loss 2.101\n",
      "63 : 92.1875 97.65625\n",
      "* Acc@1 81.064 Acc@5 96.032 loss 2.098\n",
      "64 : 93.75 98.828125\n",
      "* Acc@1 81.262 Acc@5 96.075 loss 2.094\n",
      "65 : 83.203125 96.484375\n",
      "* Acc@1 81.292 Acc@5 96.082 loss 2.093\n",
      "66 : 88.671875 98.4375\n",
      "* Acc@1 81.404 Acc@5 96.117 loss 2.089\n",
      "67 : 85.9375 96.09375\n",
      "* Acc@1 81.472 Acc@5 96.117 loss 2.086\n",
      "68 : 82.03125 98.828125\n",
      "* Acc@1 81.480 Acc@5 96.157 loss 2.085\n",
      "69 : 83.59375 100.0\n",
      "* Acc@1 81.510 Acc@5 96.213 loss 2.082\n",
      "70 : 74.609375 96.484375\n",
      "* Acc@1 81.412 Acc@5 96.217 loss 2.084\n",
      "71 : 80.078125 97.65625\n",
      "* Acc@1 81.393 Acc@5 96.237 loss 2.085\n",
      "Test ANN :  [ 70/196]  eta: 0:00:24  loss: 2.0800 (2.0850)  acc1: 80.8594 (81.3930)  acc5: 96.4844 (96.2368)  time: 0.1530  data: 0.0407  max mem: 1402\n",
      "72 : 89.84375 97.265625\n",
      "* Acc@1 81.510 Acc@5 96.251 loss 2.083\n",
      "73 : 80.859375 96.09375\n",
      "* Acc@1 81.501 Acc@5 96.249 loss 2.086\n",
      "74 : 83.984375 92.96875\n",
      "* Acc@1 81.535 Acc@5 96.205 loss 2.088\n",
      "75 : 64.453125 94.140625\n",
      "* Acc@1 81.307 Acc@5 96.177 loss 2.094\n",
      "76 : 78.515625 97.65625\n",
      "* Acc@1 81.271 Acc@5 96.197 loss 2.095\n",
      "77 : 86.328125 97.65625\n",
      "* Acc@1 81.336 Acc@5 96.216 loss 2.093\n",
      "78 : 85.9375 92.96875\n",
      "* Acc@1 81.395 Acc@5 96.174 loss 2.093\n",
      "79 : 75.0 92.96875\n",
      "* Acc@1 81.314 Acc@5 96.133 loss 2.095\n",
      "80 : 77.734375 94.921875\n",
      "* Acc@1 81.270 Acc@5 96.118 loss 2.098\n",
      "81 : 70.3125 93.75\n",
      "* Acc@1 81.134 Acc@5 96.089 loss 2.100\n",
      "Test ANN :  [ 80/196]  eta: 0:00:22  loss: 2.1193 (2.0998)  acc1: 82.0312 (81.1343)  acc5: 96.4844 (96.0889)  time: 0.1599  data: 0.0475  max mem: 1402\n",
      "82 : 77.34375 93.75\n",
      "* Acc@1 81.088 Acc@5 96.060 loss 2.102\n",
      "83 : 77.734375 96.484375\n",
      "* Acc@1 81.048 Acc@5 96.066 loss 2.102\n",
      "84 : 83.984375 96.09375\n",
      "* Acc@1 81.083 Acc@5 96.066 loss 2.101\n",
      "85 : 78.90625 94.921875\n",
      "* Acc@1 81.057 Acc@5 96.052 loss 2.102\n",
      "86 : 71.484375 90.234375\n",
      "* Acc@1 80.946 Acc@5 95.985 loss 2.105\n",
      "87 : 75.0 92.96875\n",
      "* Acc@1 80.877 Acc@5 95.950 loss 2.106\n",
      "88 : 76.953125 93.75\n",
      "* Acc@1 80.833 Acc@5 95.925 loss 2.107\n",
      "89 : 73.046875 91.796875\n",
      "* Acc@1 80.745 Acc@5 95.879 loss 2.109\n",
      "90 : 78.515625 90.234375\n",
      "* Acc@1 80.720 Acc@5 95.816 loss 2.111\n",
      "91 : 60.9375 90.234375\n",
      "* Acc@1 80.503 Acc@5 95.755 loss 2.116\n",
      "Test ANN :  [ 90/196]  eta: 0:00:20  loss: 2.2511 (2.1164)  acc1: 77.7344 (80.5031)  acc5: 93.7500 (95.7546)  time: 0.1631  data: 0.0507  max mem: 1402\n",
      "92 : 76.5625 93.359375\n",
      "* Acc@1 80.460 Acc@5 95.729 loss 2.117\n",
      "93 : 85.9375 96.875\n",
      "* Acc@1 80.519 Acc@5 95.741 loss 2.115\n",
      "94 : 72.65625 93.75\n",
      "* Acc@1 80.436 Acc@5 95.720 loss 2.117\n",
      "95 : 60.15625 92.1875\n",
      "* Acc@1 80.222 Acc@5 95.683 loss 2.121\n",
      "96 : 67.1875 91.015625\n",
      "* Acc@1 80.086 Acc@5 95.634 loss 2.126\n",
      "97 : 69.921875 91.015625\n",
      "* Acc@1 79.981 Acc@5 95.586 loss 2.129\n",
      "98 : 71.875 91.015625\n",
      "* Acc@1 79.899 Acc@5 95.540 loss 2.132\n",
      "99 : 69.140625 90.625\n",
      "* Acc@1 79.790 Acc@5 95.490 loss 2.135\n",
      "100 : 76.171875 92.1875\n",
      "* Acc@1 79.754 Acc@5 95.457 loss 2.136\n",
      "101 : 71.09375 93.359375\n",
      "* Acc@1 79.668 Acc@5 95.436 loss 2.137\n",
      "Test ANN :  [100/196]  eta: 0:00:17  loss: 2.2552 (2.1370)  acc1: 73.0469 (79.6682)  acc5: 92.1875 (95.4363)  time: 0.1608  data: 0.0482  max mem: 1402\n",
      "102 : 80.859375 96.09375\n",
      "* Acc@1 79.680 Acc@5 95.443 loss 2.136\n",
      "103 : 72.65625 93.75\n",
      "* Acc@1 79.612 Acc@5 95.426 loss 2.136\n",
      "104 : 73.4375 90.625\n",
      "* Acc@1 79.552 Acc@5 95.380 loss 2.139\n",
      "105 : 82.421875 96.09375\n",
      "* Acc@1 79.580 Acc@5 95.387 loss 2.138\n",
      "106 : 71.484375 90.625\n",
      "* Acc@1 79.503 Acc@5 95.342 loss 2.141\n",
      "107 : 79.296875 94.53125\n",
      "* Acc@1 79.501 Acc@5 95.334 loss 2.141\n",
      "108 : 73.046875 90.625\n",
      "* Acc@1 79.442 Acc@5 95.291 loss 2.142\n",
      "109 : 83.984375 94.53125\n",
      "* Acc@1 79.483 Acc@5 95.284 loss 2.141\n",
      "110 : 78.90625 93.359375\n",
      "* Acc@1 79.478 Acc@5 95.266 loss 2.141\n",
      "111 : 80.46875 95.703125\n",
      "* Acc@1 79.487 Acc@5 95.270 loss 2.140\n",
      "Test ANN :  [110/196]  eta: 0:00:15  loss: 2.1868 (2.1405)  acc1: 73.0469 (79.4869)  acc5: 93.3594 (95.2703)  time: 0.1575  data: 0.0448  max mem: 1402\n",
      "112 : 83.984375 96.09375\n",
      "* Acc@1 79.527 Acc@5 95.278 loss 2.139\n",
      "113 : 87.109375 96.09375\n",
      "* Acc@1 79.594 Acc@5 95.285 loss 2.137\n",
      "114 : 75.0 94.140625\n",
      "* Acc@1 79.554 Acc@5 95.275 loss 2.138\n",
      "115 : 70.703125 86.328125\n",
      "* Acc@1 79.477 Acc@5 95.197 loss 2.141\n",
      "116 : 72.65625 90.234375\n",
      "* Acc@1 79.418 Acc@5 95.154 loss 2.142\n",
      "117 : 76.171875 91.796875\n",
      "* Acc@1 79.390 Acc@5 95.126 loss 2.143\n",
      "118 : 73.046875 92.96875\n",
      "* Acc@1 79.337 Acc@5 95.107 loss 2.144\n",
      "119 : 86.328125 94.53125\n",
      "* Acc@1 79.395 Acc@5 95.102 loss 2.143\n",
      "120 : 87.890625 96.875\n",
      "* Acc@1 79.466 Acc@5 95.117 loss 2.140\n",
      "121 : 72.265625 89.84375\n",
      "* Acc@1 79.407 Acc@5 95.074 loss 2.141\n",
      "Test ANN :  [120/196]  eta: 0:00:13  loss: 2.1531 (2.1415)  acc1: 76.1719 (79.4066)  acc5: 93.7500 (95.0736)  time: 0.1573  data: 0.0445  max mem: 1402\n",
      "122 : 58.984375 85.9375\n",
      "* Acc@1 79.239 Acc@5 94.999 loss 2.146\n",
      "123 : 84.765625 94.921875\n",
      "* Acc@1 79.284 Acc@5 94.998 loss 2.145\n",
      "124 : 71.484375 89.84375\n",
      "* Acc@1 79.221 Acc@5 94.957 loss 2.148\n",
      "125 : 62.5 94.53125\n",
      "* Acc@1 79.088 Acc@5 94.953 loss 2.149\n",
      "126 : 82.03125 94.921875\n",
      "* Acc@1 79.111 Acc@5 94.953 loss 2.148\n",
      "127 : 82.8125 94.140625\n",
      "* Acc@1 79.140 Acc@5 94.946 loss 2.147\n",
      "128 : 70.703125 90.625\n",
      "* Acc@1 79.074 Acc@5 94.913 loss 2.150\n",
      "129 : 62.109375 91.40625\n",
      "* Acc@1 78.943 Acc@5 94.886 loss 2.153\n",
      "130 : 59.765625 93.359375\n",
      "* Acc@1 78.795 Acc@5 94.874 loss 2.156\n",
      "131 : 84.765625 96.875\n",
      "* Acc@1 78.841 Acc@5 94.889 loss 2.154\n",
      "Test ANN :  [130/196]  eta: 0:00:11  loss: 2.2840 (2.1538)  acc1: 73.0469 (78.8406)  acc5: 93.3594 (94.8891)  time: 0.1571  data: 0.0442  max mem: 1402\n",
      "132 : 76.171875 94.53125\n",
      "* Acc@1 78.820 Acc@5 94.886 loss 2.154\n",
      "133 : 73.828125 88.28125\n",
      "* Acc@1 78.783 Acc@5 94.837 loss 2.156\n",
      "134 : 75.0 92.578125\n",
      "* Acc@1 78.755 Acc@5 94.820 loss 2.158\n",
      "135 : 70.3125 89.84375\n",
      "* Acc@1 78.692 Acc@5 94.783 loss 2.159\n",
      "136 : 73.4375 95.3125\n",
      "* Acc@1 78.653 Acc@5 94.787 loss 2.160\n",
      "137 : 78.125 91.796875\n",
      "* Acc@1 78.650 Acc@5 94.765 loss 2.160\n",
      "138 : 76.171875 94.53125\n",
      "* Acc@1 78.632 Acc@5 94.763 loss 2.161\n",
      "139 : 72.65625 94.140625\n",
      "* Acc@1 78.589 Acc@5 94.759 loss 2.162\n",
      "140 : 83.203125 94.921875\n",
      "* Acc@1 78.622 Acc@5 94.760 loss 2.161\n",
      "141 : 77.734375 93.359375\n",
      "* Acc@1 78.615 Acc@5 94.750 loss 2.161\n",
      "Test ANN :  [140/196]  eta: 0:00:09  loss: 2.2520 (2.1614)  acc1: 73.8281 (78.6154)  acc5: 93.3594 (94.7501)  time: 0.1560  data: 0.0429  max mem: 1402\n",
      "142 : 81.640625 94.53125\n",
      "* Acc@1 78.637 Acc@5 94.749 loss 2.161\n",
      "143 : 67.96875 85.546875\n",
      "* Acc@1 78.562 Acc@5 94.684 loss 2.164\n",
      "144 : 76.171875 92.96875\n",
      "* Acc@1 78.545 Acc@5 94.672 loss 2.165\n",
      "145 : 77.34375 91.796875\n",
      "* Acc@1 78.537 Acc@5 94.652 loss 2.165\n",
      "146 : 68.359375 93.359375\n",
      "* Acc@1 78.467 Acc@5 94.644 loss 2.166\n",
      "147 : 73.046875 92.96875\n",
      "* Acc@1 78.431 Acc@5 94.632 loss 2.166\n",
      "148 : 77.34375 95.3125\n",
      "* Acc@1 78.423 Acc@5 94.637 loss 2.167\n",
      "149 : 78.90625 92.578125\n",
      "* Acc@1 78.426 Acc@5 94.623 loss 2.167\n",
      "150 : 75.78125 92.1875\n",
      "* Acc@1 78.409 Acc@5 94.607 loss 2.168\n",
      "151 : 76.953125 93.75\n",
      "* Acc@1 78.399 Acc@5 94.601 loss 2.168\n",
      "Test ANN :  [150/196]  eta: 0:00:08  loss: 2.2262 (2.1682)  acc1: 76.1719 (78.3992)  acc5: 92.9688 (94.6011)  time: 0.1543  data: 0.0411  max mem: 1402\n",
      "152 : 73.828125 90.234375\n",
      "* Acc@1 78.369 Acc@5 94.572 loss 2.169\n",
      "153 : 75.0 92.96875\n",
      "* Acc@1 78.347 Acc@5 94.562 loss 2.169\n",
      "154 : 81.25 93.359375\n",
      "* Acc@1 78.366 Acc@5 94.554 loss 2.169\n",
      "155 : 79.296875 96.09375\n",
      "* Acc@1 78.372 Acc@5 94.564 loss 2.168\n",
      "156 : 77.734375 94.53125\n",
      "* Acc@1 78.368 Acc@5 94.564 loss 2.169\n",
      "157 : 79.296875 92.578125\n",
      "* Acc@1 78.374 Acc@5 94.551 loss 2.169\n",
      "158 : 77.734375 94.140625\n",
      "* Acc@1 78.370 Acc@5 94.549 loss 2.169\n",
      "159 : 63.28125 91.015625\n",
      "* Acc@1 78.275 Acc@5 94.526 loss 2.171\n",
      "160 : 74.609375 96.09375\n",
      "* Acc@1 78.252 Acc@5 94.536 loss 2.171\n",
      "161 : 84.375 95.703125\n",
      "* Acc@1 78.290 Acc@5 94.543 loss 2.170\n",
      "Test ANN :  [160/196]  eta: 0:00:06  loss: 2.2024 (2.1699)  acc1: 76.9531 (78.2900)  acc5: 92.9688 (94.5434)  time: 0.1507  data: 0.0375  max mem: 1402\n",
      "162 : 74.21875 92.96875\n",
      "* Acc@1 78.265 Acc@5 94.534 loss 2.171\n",
      "163 : 81.25 92.578125\n",
      "* Acc@1 78.283 Acc@5 94.522 loss 2.170\n",
      "164 : 51.953125 87.5\n",
      "* Acc@1 78.123 Acc@5 94.479 loss 2.173\n",
      "165 : 74.609375 94.921875\n",
      "* Acc@1 78.101 Acc@5 94.482 loss 2.174\n",
      "166 : 66.40625 94.53125\n",
      "* Acc@1 78.031 Acc@5 94.482 loss 2.175\n",
      "167 : 84.765625 96.875\n",
      "* Acc@1 78.071 Acc@5 94.496 loss 2.173\n",
      "168 : 74.21875 92.578125\n",
      "* Acc@1 78.048 Acc@5 94.485 loss 2.174\n",
      "169 : 70.3125 94.53125\n",
      "* Acc@1 78.002 Acc@5 94.485 loss 2.175\n",
      "170 : 80.078125 93.75\n",
      "* Acc@1 78.015 Acc@5 94.481 loss 2.174\n",
      "171 : 83.59375 97.65625\n",
      "* Acc@1 78.047 Acc@5 94.499 loss 2.173\n",
      "Test ANN :  [170/196]  eta: 0:00:04  loss: 2.1873 (2.1729)  acc1: 75.0000 (78.0473)  acc5: 93.7500 (94.4993)  time: 0.1522  data: 0.0390  max mem: 1402\n",
      "172 : 71.875 94.140625\n",
      "* Acc@1 78.011 Acc@5 94.497 loss 2.173\n",
      "173 : 74.21875 90.625\n",
      "* Acc@1 77.990 Acc@5 94.475 loss 2.174\n",
      "174 : 79.296875 92.1875\n",
      "* Acc@1 77.997 Acc@5 94.462 loss 2.174\n",
      "175 : 76.171875 94.53125\n",
      "* Acc@1 77.987 Acc@5 94.462 loss 2.174\n",
      "176 : 71.875 91.015625\n",
      "* Acc@1 77.952 Acc@5 94.442 loss 2.176\n",
      "177 : 77.34375 91.015625\n",
      "* Acc@1 77.948 Acc@5 94.423 loss 2.176\n",
      "178 : 54.296875 90.625\n",
      "* Acc@1 77.816 Acc@5 94.402 loss 2.179\n",
      "179 : 82.03125 97.65625\n",
      "* Acc@1 77.839 Acc@5 94.420 loss 2.178\n",
      "180 : 80.46875 93.359375\n",
      "* Acc@1 77.854 Acc@5 94.414 loss 2.178\n",
      "181 : 72.65625 96.09375\n",
      "* Acc@1 77.825 Acc@5 94.423 loss 2.179\n",
      "Test ANN :  [180/196]  eta: 0:00:02  loss: 2.2275 (2.1789)  acc1: 74.2188 (77.8250)  acc5: 93.3594 (94.4233)  time: 0.1530  data: 0.0396  max mem: 1402\n",
      "182 : 81.25 96.875\n",
      "* Acc@1 77.844 Acc@5 94.437 loss 2.178\n",
      "183 : 86.328125 95.3125\n",
      "* Acc@1 77.890 Acc@5 94.442 loss 2.177\n",
      "184 : 85.15625 98.046875\n",
      "* Acc@1 77.930 Acc@5 94.461 loss 2.176\n",
      "185 : 82.421875 96.875\n",
      "* Acc@1 77.954 Acc@5 94.474 loss 2.176\n",
      "186 : 78.125 98.828125\n",
      "* Acc@1 77.955 Acc@5 94.498 loss 2.175\n",
      "187 : 92.96875 98.046875\n",
      "* Acc@1 78.035 Acc@5 94.517 loss 2.173\n",
      "188 : 72.265625 91.796875\n",
      "* Acc@1 78.004 Acc@5 94.502 loss 2.174\n",
      "189 : 76.953125 90.625\n",
      "* Acc@1 77.999 Acc@5 94.482 loss 2.174\n",
      "190 : 65.625 92.1875\n",
      "* Acc@1 77.934 Acc@5 94.470 loss 2.176\n",
      "191 : 70.3125 96.875\n",
      "* Acc@1 77.894 Acc@5 94.482 loss 2.177\n",
      "Test ANN :  [190/196]  eta: 0:00:01  loss: 2.1823 (2.1770)  acc1: 76.9531 (77.8939)  acc5: 94.1406 (94.4822)  time: 0.1485  data: 0.0351  max mem: 1402\n",
      "192 : 80.078125 94.140625\n",
      "* Acc@1 77.905 Acc@5 94.480 loss 2.176\n",
      "193 : 88.671875 99.21875\n",
      "* Acc@1 77.961 Acc@5 94.505 loss 2.175\n",
      "194 : 94.921875 99.21875\n",
      "* Acc@1 78.048 Acc@5 94.529 loss 2.173\n",
      "195 : 84.375 96.09375\n",
      "* Acc@1 78.081 Acc@5 94.537 loss 2.172\n",
      "196 : 51.25 82.5\n",
      "* Acc@1 78.038 Acc@5 94.518 loss 2.176\n",
      "Test ANN :  [195/196]  eta: 0:00:00  loss: 2.0556 (2.1762)  acc1: 80.0781 (78.0380)  acc5: 96.0938 (94.5180)  time: 0.1591  data: 0.0472  max mem: 1402\n",
      "Test ANN : Total time: 0:00:33 (0.1730 s / it)\n",
      "* Acc@1 78.038 Acc@5 94.518 loss 2.176\n"
     ]
    }
   ],
   "source": [
    "!python run_class_finetuning.py --eval_data_path ../data/val --nb_classes 1000 --data_set image_folder --model vit_small_patch16_224 --model_path ../models/original_model/vit_small_patch16_224.pth --input_size 224 --batch_size 256 --test_mode ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952ed277-7182-4239-901d-7698d6450dd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:343: UserWarning: Overwriting vit_tiny_patch16_224 in registry with model_vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:353: UserWarning: Overwriting vit_tiny_patch16_384 in registry with model_vit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:386: UserWarning: Overwriting vit_small_patch32_224 in registry with model_vit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:398: UserWarning: Overwriting vit_small_patch32_384 in registry with model_vit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:410: UserWarning: Overwriting vit_small_patch16_224 in registry with model_vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:421: UserWarning: Overwriting vit_small_patch16_384 in registry with model_vit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:454: UserWarning: Overwriting vit_base_patch32_224 in registry with model_vit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained: bool = False, **kwargs) -> VisionTransformer:\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:465: UserWarning: Overwriting vit_base_patch32_384 in registry with model_vit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:476: UserWarning: Overwriting vit_base_patch16_224 in registry with model_vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:509: UserWarning: Overwriting vit_base_patch16_384 in registry with model_vit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:520: UserWarning: Overwriting vit_large_patch32_224 in registry with model_vit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:531: UserWarning: Overwriting vit_large_patch32_384 in registry with model_vit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:542: UserWarning: Overwriting vit_large_patch16_224 in registry with model_vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:553: UserWarning: Overwriting vit_large_patch16_384 in registry with model_vit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "Namespace(batch_size=256,\n",
      "data_set='image_folder',\n",
      "device='cuda',\n",
      "distributed=False,\n",
      "eval_data_path='../data/val',\n",
      "imagenet_default_mean_and_std=False,\n",
      "input_size=224,\n",
      "linear_num=8,\n",
      "model='vit_base_patch16_224',\n",
      "model_path='../models/original_model/vit_base_patch16_224.pth',\n",
      "monitor=True,\n",
      "nb_classes=1000,\n",
      "num_workers=10,\n",
      "output_dir='../models',\n",
      "percent=0.99,\n",
      "qkv_num=8,\n",
      "savename='test',\n",
      "seed=0,\n",
      "softmax_num=8,\n",
      "softmax_p=0.0125,\n",
      "test_T=8,\n",
      "test_mode='ann')\n",
      "Number of the class = 1000\n",
      " patchembed dropout sequential layernorm identity linear \n",
      "patch_embed conv2d identity \n",
      "blocks block block block block block block block block block block block block \n",
      "0 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "1 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "2 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "3 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "4 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "5 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "6 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "7 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "8 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "9 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "10 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "11 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "Load ckpt from ../models/original_model/vit_base_patch16_224.pth\n",
      "Load state_dict by model_key = model\n",
      "number of params: 86567656\n",
      "Batch size = 256\n",
      "1 : 95.3125 99.21875\n",
      "* Acc@1 95.312 Acc@5 99.219 loss 1.241\n",
      "Test ANN :  [  0/196]  eta: 0:15:15  loss: 1.2412 (1.2412)  acc1: 95.3125 (95.3125)  acc5: 99.2188 (99.2188)  time: 4.6730  data: 3.3382  max mem: 2742\n",
      "2 : 89.0625 97.265625\n",
      "* Acc@1 92.188 Acc@5 98.242 loss 1.343\n",
      "3 : 97.65625 100.0\n",
      "* Acc@1 94.010 Acc@5 98.828 loss 1.265\n",
      "4 : 94.53125 99.21875\n",
      "* Acc@1 94.141 Acc@5 98.926 loss 1.251\n",
      "5 : 94.921875 98.828125\n",
      "* Acc@1 94.297 Acc@5 98.906 loss 1.249\n",
      "6 : 84.375 97.265625\n",
      "* Acc@1 92.643 Acc@5 98.633 loss 1.299\n",
      "7 : 76.171875 96.875\n",
      "* Acc@1 90.290 Acc@5 98.382 loss 1.364\n",
      "8 : 76.953125 99.609375\n",
      "* Acc@1 88.623 Acc@5 98.535 loss 1.394\n",
      "9 : 83.984375 95.3125\n",
      "* Acc@1 88.108 Acc@5 98.177 loss 1.415\n",
      "10 : 82.421875 96.484375\n",
      "* Acc@1 87.539 Acc@5 98.008 loss 1.431\n",
      "11 : 80.078125 95.703125\n",
      "* Acc@1 86.861 Acc@5 97.798 loss 1.459\n",
      "Test ANN :  [ 10/196]  eta: 0:02:04  loss: 1.5516 (1.4588)  acc1: 84.3750 (86.8608)  acc5: 97.2656 (97.7983)  time: 0.6704  data: 0.3037  max mem: 2743\n",
      "12 : 78.90625 96.09375\n",
      "* Acc@1 86.198 Acc@5 97.656 loss 1.485\n",
      "13 : 76.5625 93.75\n",
      "* Acc@1 85.457 Acc@5 97.356 loss 1.511\n",
      "14 : 83.984375 96.09375\n",
      "* Acc@1 85.352 Acc@5 97.266 loss 1.508\n",
      "15 : 78.125 97.65625\n",
      "* Acc@1 84.870 Acc@5 97.292 loss 1.514\n",
      "16 : 83.203125 97.65625\n",
      "* Acc@1 84.766 Acc@5 97.314 loss 1.521\n",
      "17 : 92.96875 99.21875\n",
      "* Acc@1 85.248 Acc@5 97.426 loss 1.506\n",
      "18 : 98.046875 99.21875\n",
      "* Acc@1 85.959 Acc@5 97.526 loss 1.485\n",
      "19 : 96.09375 98.4375\n",
      "* Acc@1 86.493 Acc@5 97.574 loss 1.471\n",
      "20 : 92.578125 100.0\n",
      "* Acc@1 86.797 Acc@5 97.695 loss 1.459\n",
      "21 : 91.015625 97.265625\n",
      "* Acc@1 86.998 Acc@5 97.675 loss 1.458\n",
      "Test ANN :  [ 20/196]  eta: 0:01:24  loss: 1.4696 (1.4576)  acc1: 83.9844 (86.9978)  acc5: 97.2656 (97.6749)  time: 0.2703  data: 0.0002  max mem: 2743\n",
      "22 : 89.0625 98.046875\n",
      "* Acc@1 87.092 Acc@5 97.692 loss 1.457\n",
      "23 : 91.796875 96.09375\n",
      "* Acc@1 87.296 Acc@5 97.622 loss 1.454\n",
      "24 : 80.859375 98.4375\n",
      "* Acc@1 87.028 Acc@5 97.656 loss 1.461\n",
      "25 : 89.0625 95.703125\n",
      "* Acc@1 87.109 Acc@5 97.578 loss 1.462\n",
      "26 : 95.703125 99.609375\n",
      "* Acc@1 87.440 Acc@5 97.656 loss 1.451\n",
      "27 : 92.96875 99.609375\n",
      "* Acc@1 87.645 Acc@5 97.729 loss 1.442\n",
      "28 : 95.703125 99.21875\n",
      "* Acc@1 87.932 Acc@5 97.782 loss 1.432\n",
      "29 : 97.65625 99.609375\n",
      "* Acc@1 88.268 Acc@5 97.845 loss 1.421\n",
      "30 : 86.328125 98.4375\n",
      "* Acc@1 88.203 Acc@5 97.865 loss 1.422\n",
      "31 : 85.546875 95.3125\n",
      "* Acc@1 88.117 Acc@5 97.782 loss 1.427\n",
      "Test ANN :  [ 30/196]  eta: 0:01:08  loss: 1.4275 (1.4268)  acc1: 89.0625 (88.1174)  acc5: 98.0469 (97.7823)  time: 0.2722  data: 0.0018  max mem: 2743\n",
      "32 : 80.46875 96.875\n",
      "* Acc@1 87.878 Acc@5 97.754 loss 1.436\n",
      "33 : 65.625 93.75\n",
      "* Acc@1 87.204 Acc@5 97.633 loss 1.456\n",
      "34 : 84.765625 96.484375\n",
      "* Acc@1 87.132 Acc@5 97.599 loss 1.460\n",
      "35 : 87.109375 97.265625\n",
      "* Acc@1 87.132 Acc@5 97.589 loss 1.460\n",
      "36 : 82.421875 97.65625\n",
      "* Acc@1 87.001 Acc@5 97.591 loss 1.462\n",
      "37 : 77.734375 96.875\n",
      "* Acc@1 86.750 Acc@5 97.572 loss 1.470\n",
      "38 : 81.25 93.359375\n",
      "* Acc@1 86.606 Acc@5 97.461 loss 1.477\n",
      "39 : 78.515625 98.4375\n",
      "* Acc@1 86.398 Acc@5 97.486 loss 1.482\n",
      "40 : 79.6875 98.4375\n",
      "* Acc@1 86.230 Acc@5 97.510 loss 1.487\n",
      "41 : 87.109375 96.484375\n",
      "* Acc@1 86.252 Acc@5 97.485 loss 1.488\n",
      "Test ANN :  [ 40/196]  eta: 0:01:00  loss: 1.5314 (1.4883)  acc1: 85.5469 (86.2519)  acc5: 97.2656 (97.4848)  time: 0.2867  data: 0.0162  max mem: 2743\n",
      "42 : 86.328125 96.09375\n",
      "* Acc@1 86.254 Acc@5 97.452 loss 1.488\n",
      "43 : 87.5 98.4375\n",
      "* Acc@1 86.283 Acc@5 97.475 loss 1.487\n",
      "44 : 82.421875 94.921875\n",
      "* Acc@1 86.195 Acc@5 97.417 loss 1.491\n",
      "45 : 80.859375 95.3125\n",
      "* Acc@1 86.076 Acc@5 97.370 loss 1.497\n",
      "46 : 82.421875 96.875\n",
      "* Acc@1 85.997 Acc@5 97.359 loss 1.498\n",
      "47 : 74.21875 97.65625\n",
      "* Acc@1 85.746 Acc@5 97.365 loss 1.504\n",
      "48 : 83.203125 98.828125\n",
      "* Acc@1 85.693 Acc@5 97.396 loss 1.505\n",
      "49 : 73.828125 97.65625\n",
      "* Acc@1 85.451 Acc@5 97.401 loss 1.510\n",
      "50 : 94.921875 98.4375\n",
      "* Acc@1 85.641 Acc@5 97.422 loss 1.505\n",
      "51 : 90.234375 98.4375\n",
      "* Acc@1 85.731 Acc@5 97.442 loss 1.503\n",
      "Test ANN :  [ 50/196]  eta: 0:00:53  loss: 1.6086 (1.5025)  acc1: 82.4219 (85.7307)  acc5: 96.8750 (97.4418)  time: 0.3002  data: 0.0297  max mem: 2743\n",
      "52 : 83.59375 98.4375\n",
      "* Acc@1 85.690 Acc@5 97.461 loss 1.503\n",
      "53 : 81.25 98.4375\n",
      "* Acc@1 85.606 Acc@5 97.479 loss 1.503\n",
      "54 : 82.8125 95.3125\n",
      "* Acc@1 85.554 Acc@5 97.439 loss 1.508\n",
      "55 : 77.734375 97.265625\n",
      "* Acc@1 85.412 Acc@5 97.436 loss 1.512\n",
      "56 : 72.265625 97.265625\n",
      "* Acc@1 85.177 Acc@5 97.433 loss 1.517\n",
      "57 : 87.890625 98.4375\n",
      "* Acc@1 85.225 Acc@5 97.451 loss 1.516\n",
      "58 : 94.140625 98.4375\n",
      "* Acc@1 85.379 Acc@5 97.468 loss 1.512\n",
      "59 : 87.109375 97.265625\n",
      "* Acc@1 85.408 Acc@5 97.464 loss 1.513\n",
      "60 : 83.59375 99.21875\n",
      "* Acc@1 85.378 Acc@5 97.493 loss 1.513\n",
      "61 : 85.546875 97.65625\n",
      "* Acc@1 85.380 Acc@5 97.496 loss 1.513\n",
      "Test ANN :  [ 60/196]  eta: 0:00:48  loss: 1.5508 (1.5134)  acc1: 83.2031 (85.3804)  acc5: 97.6562 (97.4962)  time: 0.2996  data: 0.0288  max mem: 2743\n",
      "62 : 87.890625 96.875\n",
      "* Acc@1 85.421 Acc@5 97.486 loss 1.513\n",
      "63 : 91.796875 99.21875\n",
      "* Acc@1 85.522 Acc@5 97.514 loss 1.509\n",
      "64 : 96.484375 99.609375\n",
      "* Acc@1 85.693 Acc@5 97.546 loss 1.504\n",
      "65 : 88.671875 98.828125\n",
      "* Acc@1 85.739 Acc@5 97.566 loss 1.502\n",
      "66 : 94.140625 98.828125\n",
      "* Acc@1 85.866 Acc@5 97.585 loss 1.499\n",
      "67 : 87.109375 98.4375\n",
      "* Acc@1 85.885 Acc@5 97.598 loss 1.498\n",
      "68 : 84.765625 99.609375\n",
      "* Acc@1 85.869 Acc@5 97.628 loss 1.495\n",
      "69 : 85.9375 99.609375\n",
      "* Acc@1 85.870 Acc@5 97.656 loss 1.493\n",
      "70 : 80.078125 98.828125\n",
      "* Acc@1 85.787 Acc@5 97.673 loss 1.495\n",
      "71 : 85.15625 98.4375\n",
      "* Acc@1 85.778 Acc@5 97.684 loss 1.496\n",
      "Test ANN :  [ 70/196]  eta: 0:00:44  loss: 1.4925 (1.4955)  acc1: 85.5469 (85.7779)  acc5: 98.4375 (97.6838)  time: 0.3004  data: 0.0289  max mem: 2743\n",
      "72 : 92.96875 97.65625\n",
      "* Acc@1 85.878 Acc@5 97.683 loss 1.494\n",
      "73 : 84.375 96.484375\n",
      "* Acc@1 85.857 Acc@5 97.667 loss 1.496\n",
      "74 : 86.328125 93.75\n",
      "* Acc@1 85.864 Acc@5 97.614 loss 1.498\n",
      "75 : 67.96875 96.09375\n",
      "* Acc@1 85.625 Acc@5 97.594 loss 1.505\n",
      "76 : 82.03125 98.828125\n",
      "* Acc@1 85.578 Acc@5 97.610 loss 1.505\n",
      "77 : 91.40625 98.4375\n",
      "* Acc@1 85.653 Acc@5 97.621 loss 1.503\n",
      "78 : 93.359375 97.265625\n",
      "* Acc@1 85.752 Acc@5 97.616 loss 1.501\n",
      "79 : 79.6875 95.3125\n",
      "* Acc@1 85.675 Acc@5 97.587 loss 1.503\n",
      "80 : 82.8125 97.265625\n",
      "* Acc@1 85.640 Acc@5 97.583 loss 1.504\n",
      "81 : 76.953125 95.3125\n",
      "* Acc@1 85.532 Acc@5 97.555 loss 1.507\n",
      "Test ANN :  [ 80/196]  eta: 0:00:39  loss: 1.4164 (1.5075)  acc1: 85.9375 (85.5324)  acc5: 98.4375 (97.5550)  time: 0.3019  data: 0.0300  max mem: 2743\n",
      "82 : 81.640625 96.484375\n",
      "* Acc@1 85.485 Acc@5 97.542 loss 1.510\n",
      "83 : 85.15625 98.4375\n",
      "* Acc@1 85.481 Acc@5 97.553 loss 1.510\n",
      "84 : 89.453125 96.875\n",
      "* Acc@1 85.528 Acc@5 97.545 loss 1.509\n",
      "85 : 85.9375 97.65625\n",
      "* Acc@1 85.533 Acc@5 97.546 loss 1.508\n",
      "86 : 78.515625 93.359375\n",
      "* Acc@1 85.451 Acc@5 97.497 loss 1.512\n",
      "87 : 79.296875 96.875\n",
      "* Acc@1 85.381 Acc@5 97.490 loss 1.514\n",
      "88 : 84.765625 96.875\n",
      "* Acc@1 85.374 Acc@5 97.483 loss 1.515\n",
      "89 : 79.6875 92.96875\n",
      "* Acc@1 85.310 Acc@5 97.432 loss 1.517\n",
      "90 : 84.765625 95.3125\n",
      "* Acc@1 85.304 Acc@5 97.409 loss 1.518\n",
      "91 : 69.140625 94.921875\n",
      "* Acc@1 85.126 Acc@5 97.382 loss 1.523\n",
      "Test ANN :  [ 90/196]  eta: 0:00:35  loss: 1.6075 (1.5234)  acc1: 82.8125 (85.1262)  acc5: 96.4844 (97.3815)  time: 0.3023  data: 0.0300  max mem: 2743\n",
      "92 : 82.421875 97.265625\n",
      "* Acc@1 85.097 Acc@5 97.380 loss 1.525\n",
      "93 : 91.015625 98.046875\n",
      "* Acc@1 85.160 Acc@5 97.387 loss 1.523\n",
      "94 : 77.34375 94.921875\n",
      "* Acc@1 85.077 Acc@5 97.361 loss 1.527\n",
      "95 : 67.578125 95.3125\n",
      "* Acc@1 84.893 Acc@5 97.340 loss 1.531\n",
      "96 : 73.828125 94.921875\n",
      "* Acc@1 84.778 Acc@5 97.314 loss 1.535\n",
      "97 : 79.6875 94.53125\n",
      "* Acc@1 84.725 Acc@5 97.286 loss 1.537\n",
      "98 : 74.609375 90.234375\n",
      "* Acc@1 84.622 Acc@5 97.214 loss 1.542\n",
      "99 : 75.0 92.578125\n",
      "* Acc@1 84.525 Acc@5 97.167 loss 1.546\n",
      "100 : 83.59375 94.921875\n",
      "* Acc@1 84.516 Acc@5 97.145 loss 1.546\n",
      "101 : 79.296875 96.484375\n",
      "* Acc@1 84.464 Acc@5 97.138 loss 1.547\n",
      "Test ANN :  [100/196]  eta: 0:00:32  loss: 1.6628 (1.5473)  acc1: 79.6875 (84.4640)  acc5: 95.3125 (97.1380)  time: 0.3032  data: 0.0305  max mem: 2743\n",
      "102 : 81.25 96.09375\n",
      "* Acc@1 84.432 Acc@5 97.128 loss 1.548\n",
      "103 : 76.953125 96.484375\n",
      "* Acc@1 84.360 Acc@5 97.122 loss 1.549\n",
      "104 : 78.125 93.359375\n",
      "* Acc@1 84.300 Acc@5 97.085 loss 1.552\n",
      "105 : 85.546875 95.3125\n",
      "* Acc@1 84.312 Acc@5 97.068 loss 1.553\n",
      "106 : 81.25 96.484375\n",
      "* Acc@1 84.283 Acc@5 97.063 loss 1.554\n",
      "107 : 83.984375 97.265625\n",
      "* Acc@1 84.280 Acc@5 97.065 loss 1.554\n",
      "108 : 83.59375 95.3125\n",
      "* Acc@1 84.274 Acc@5 97.049 loss 1.555\n",
      "109 : 88.28125 96.09375\n",
      "* Acc@1 84.310 Acc@5 97.040 loss 1.554\n",
      "110 : 88.28125 96.875\n",
      "* Acc@1 84.347 Acc@5 97.038 loss 1.554\n",
      "111 : 85.15625 96.875\n",
      "* Acc@1 84.354 Acc@5 97.037 loss 1.553\n",
      "Test ANN :  [110/196]  eta: 0:00:28  loss: 1.6398 (1.5530)  acc1: 81.2500 (84.3539)  acc5: 95.3125 (97.0369)  time: 0.3037  data: 0.0304  max mem: 2743\n",
      "112 : 90.625 97.65625\n",
      "* Acc@1 84.410 Acc@5 97.042 loss 1.551\n",
      "113 : 90.234375 97.265625\n",
      "* Acc@1 84.461 Acc@5 97.044 loss 1.550\n",
      "114 : 81.25 95.3125\n",
      "* Acc@1 84.433 Acc@5 97.029 loss 1.552\n",
      "115 : 78.90625 95.703125\n",
      "* Acc@1 84.385 Acc@5 97.018 loss 1.554\n",
      "116 : 81.25 95.3125\n",
      "* Acc@1 84.358 Acc@5 97.003 loss 1.555\n",
      "117 : 83.203125 97.265625\n",
      "* Acc@1 84.348 Acc@5 97.005 loss 1.555\n",
      "118 : 82.421875 94.921875\n",
      "* Acc@1 84.332 Acc@5 96.988 loss 1.556\n",
      "119 : 91.015625 97.65625\n",
      "* Acc@1 84.388 Acc@5 96.993 loss 1.555\n",
      "120 : 92.96875 98.828125\n",
      "* Acc@1 84.460 Acc@5 97.008 loss 1.553\n",
      "121 : 79.296875 92.96875\n",
      "* Acc@1 84.417 Acc@5 96.975 loss 1.555\n",
      "Test ANN :  [120/196]  eta: 0:00:25  loss: 1.5799 (1.5551)  acc1: 83.2031 (84.4170)  acc5: 96.0938 (96.9751)  time: 0.3042  data: 0.0303  max mem: 2743\n",
      "122 : 60.15625 90.234375\n",
      "* Acc@1 84.218 Acc@5 96.920 loss 1.561\n",
      "123 : 86.71875 98.046875\n",
      "* Acc@1 84.238 Acc@5 96.929 loss 1.560\n",
      "124 : 75.78125 92.578125\n",
      "* Acc@1 84.170 Acc@5 96.894 loss 1.563\n",
      "125 : 66.796875 97.65625\n",
      "* Acc@1 84.031 Acc@5 96.900 loss 1.565\n",
      "126 : 87.109375 98.046875\n",
      "* Acc@1 84.056 Acc@5 96.909 loss 1.564\n",
      "127 : 89.0625 98.046875\n",
      "* Acc@1 84.095 Acc@5 96.918 loss 1.563\n",
      "128 : 78.90625 96.09375\n",
      "* Acc@1 84.055 Acc@5 96.912 loss 1.565\n",
      "129 : 70.3125 94.921875\n",
      "* Acc@1 83.948 Acc@5 96.896 loss 1.567\n",
      "130 : 72.65625 94.921875\n",
      "* Acc@1 83.861 Acc@5 96.881 loss 1.570\n",
      "131 : 90.625 98.828125\n",
      "* Acc@1 83.913 Acc@5 96.896 loss 1.568\n",
      "Test ANN :  [130/196]  eta: 0:00:21  loss: 1.6636 (1.5681)  acc1: 81.2500 (83.9128)  acc5: 96.0938 (96.8959)  time: 0.3042  data: 0.0301  max mem: 2743\n",
      "132 : 78.90625 97.265625\n",
      "* Acc@1 83.875 Acc@5 96.899 loss 1.568\n",
      "133 : 82.421875 92.578125\n",
      "* Acc@1 83.864 Acc@5 96.866 loss 1.570\n",
      "134 : 78.125 96.09375\n",
      "* Acc@1 83.821 Acc@5 96.860 loss 1.571\n",
      "135 : 80.46875 95.3125\n",
      "* Acc@1 83.796 Acc@5 96.849 loss 1.571\n",
      "136 : 79.6875 94.140625\n",
      "* Acc@1 83.766 Acc@5 96.829 loss 1.573\n",
      "137 : 81.25 97.265625\n",
      "* Acc@1 83.748 Acc@5 96.832 loss 1.573\n",
      "138 : 80.078125 97.65625\n",
      "* Acc@1 83.721 Acc@5 96.838 loss 1.574\n",
      "139 : 83.203125 95.3125\n",
      "* Acc@1 83.717 Acc@5 96.827 loss 1.574\n",
      "140 : 92.1875 98.046875\n",
      "* Acc@1 83.778 Acc@5 96.836 loss 1.573\n",
      "141 : 83.984375 96.484375\n",
      "* Acc@1 83.779 Acc@5 96.833 loss 1.573\n",
      "Test ANN :  [140/196]  eta: 0:00:18  loss: 1.6444 (1.5726)  acc1: 80.0781 (83.7794)  acc5: 96.0938 (96.8334)  time: 0.3042  data: 0.0299  max mem: 2743\n",
      "142 : 84.765625 96.09375\n",
      "* Acc@1 83.786 Acc@5 96.828 loss 1.573\n",
      "143 : 80.078125 94.53125\n",
      "* Acc@1 83.760 Acc@5 96.812 loss 1.574\n",
      "144 : 76.5625 96.484375\n",
      "* Acc@1 83.710 Acc@5 96.810 loss 1.575\n",
      "145 : 82.03125 95.3125\n",
      "* Acc@1 83.699 Acc@5 96.800 loss 1.575\n",
      "146 : 70.703125 95.703125\n",
      "* Acc@1 83.610 Acc@5 96.792 loss 1.577\n",
      "147 : 76.5625 96.484375\n",
      "* Acc@1 83.562 Acc@5 96.790 loss 1.578\n",
      "148 : 88.671875 99.21875\n",
      "* Acc@1 83.596 Acc@5 96.806 loss 1.577\n",
      "149 : 82.421875 92.96875\n",
      "* Acc@1 83.589 Acc@5 96.781 loss 1.578\n",
      "150 : 82.03125 93.75\n",
      "* Acc@1 83.578 Acc@5 96.760 loss 1.579\n",
      "151 : 84.765625 96.09375\n",
      "* Acc@1 83.586 Acc@5 96.756 loss 1.579\n",
      "Test ANN :  [150/196]  eta: 0:00:14  loss: 1.6444 (1.5788)  acc1: 81.2500 (83.5860)  acc5: 96.0938 (96.7560)  time: 0.3046  data: 0.0299  max mem: 2743\n",
      "152 : 80.46875 94.53125\n",
      "* Acc@1 83.565 Acc@5 96.741 loss 1.580\n",
      "153 : 80.46875 96.484375\n",
      "* Acc@1 83.545 Acc@5 96.740 loss 1.580\n",
      "154 : 83.984375 96.875\n",
      "* Acc@1 83.548 Acc@5 96.741 loss 1.580\n",
      "155 : 83.203125 98.4375\n",
      "* Acc@1 83.546 Acc@5 96.752 loss 1.580\n",
      "156 : 82.8125 96.09375\n",
      "* Acc@1 83.541 Acc@5 96.747 loss 1.580\n",
      "157 : 87.890625 97.265625\n",
      "* Acc@1 83.569 Acc@5 96.751 loss 1.579\n",
      "158 : 80.46875 97.265625\n",
      "* Acc@1 83.549 Acc@5 96.754 loss 1.580\n",
      "159 : 68.75 94.921875\n",
      "* Acc@1 83.456 Acc@5 96.742 loss 1.582\n",
      "160 : 72.265625 95.3125\n",
      "* Acc@1 83.386 Acc@5 96.733 loss 1.583\n",
      "161 : 91.796875 97.265625\n",
      "* Acc@1 83.438 Acc@5 96.737 loss 1.582\n",
      "Test ANN :  [160/196]  eta: 0:00:11  loss: 1.6281 (1.5823)  acc1: 82.0312 (83.4385)  acc5: 96.0938 (96.7367)  time: 0.3055  data: 0.0305  max mem: 2743\n",
      "162 : 77.34375 95.3125\n",
      "* Acc@1 83.401 Acc@5 96.728 loss 1.584\n",
      "163 : 87.890625 97.265625\n",
      "* Acc@1 83.428 Acc@5 96.731 loss 1.583\n",
      "164 : 58.59375 89.0625\n",
      "* Acc@1 83.277 Acc@5 96.684 loss 1.588\n",
      "165 : 81.640625 97.265625\n",
      "* Acc@1 83.267 Acc@5 96.688 loss 1.588\n",
      "166 : 71.484375 96.484375\n",
      "* Acc@1 83.196 Acc@5 96.687 loss 1.589\n",
      "167 : 85.9375 98.4375\n",
      "* Acc@1 83.212 Acc@5 96.697 loss 1.588\n",
      "168 : 74.609375 95.3125\n",
      "* Acc@1 83.161 Acc@5 96.689 loss 1.590\n",
      "169 : 77.734375 96.09375\n",
      "* Acc@1 83.129 Acc@5 96.685 loss 1.591\n",
      "170 : 81.640625 94.140625\n",
      "* Acc@1 83.120 Acc@5 96.670 loss 1.591\n",
      "171 : 89.84375 98.828125\n",
      "* Acc@1 83.160 Acc@5 96.683 loss 1.590\n",
      "Test ANN :  [170/196]  eta: 0:00:08  loss: 1.6281 (1.5896)  acc1: 80.4688 (83.1597)  acc5: 96.4844 (96.6831)  time: 0.3062  data: 0.0307  max mem: 2743\n",
      "172 : 74.609375 98.046875\n",
      "* Acc@1 83.110 Acc@5 96.691 loss 1.590\n",
      "173 : 74.21875 92.578125\n",
      "* Acc@1 83.059 Acc@5 96.667 loss 1.592\n",
      "174 : 84.765625 95.703125\n",
      "* Acc@1 83.068 Acc@5 96.662 loss 1.592\n",
      "175 : 82.03125 97.265625\n",
      "* Acc@1 83.062 Acc@5 96.665 loss 1.592\n",
      "176 : 76.171875 94.140625\n",
      "* Acc@1 83.023 Acc@5 96.651 loss 1.593\n",
      "177 : 78.515625 93.75\n",
      "* Acc@1 82.998 Acc@5 96.634 loss 1.594\n",
      "178 : 60.546875 95.703125\n",
      "* Acc@1 82.872 Acc@5 96.629 loss 1.597\n",
      "179 : 85.9375 98.828125\n",
      "* Acc@1 82.889 Acc@5 96.641 loss 1.597\n",
      "180 : 85.9375 98.4375\n",
      "* Acc@1 82.906 Acc@5 96.651 loss 1.596\n",
      "181 : 76.953125 96.09375\n",
      "* Acc@1 82.873 Acc@5 96.648 loss 1.597\n",
      "Test ANN :  [180/196]  eta: 0:00:05  loss: 1.6686 (1.5967)  acc1: 77.7344 (82.8729)  acc5: 96.0938 (96.6484)  time: 0.3062  data: 0.0301  max mem: 2743\n",
      "182 : 87.109375 98.828125\n",
      "* Acc@1 82.896 Acc@5 96.660 loss 1.596\n",
      "183 : 87.109375 97.265625\n",
      "* Acc@1 82.919 Acc@5 96.664 loss 1.595\n",
      "184 : 89.84375 98.828125\n",
      "* Acc@1 82.957 Acc@5 96.675 loss 1.594\n",
      "185 : 85.15625 97.265625\n",
      "* Acc@1 82.969 Acc@5 96.679 loss 1.593\n",
      "186 : 73.4375 98.4375\n",
      "* Acc@1 82.918 Acc@5 96.688 loss 1.594\n",
      "187 : 95.703125 98.828125\n",
      "* Acc@1 82.986 Acc@5 96.700 loss 1.592\n",
      "188 : 75.390625 92.578125\n",
      "* Acc@1 82.945 Acc@5 96.678 loss 1.593\n",
      "189 : 80.859375 94.921875\n",
      "* Acc@1 82.934 Acc@5 96.668 loss 1.594\n",
      "190 : 69.921875 95.703125\n",
      "* Acc@1 82.866 Acc@5 96.663 loss 1.596\n",
      "191 : 73.828125 97.65625\n",
      "* Acc@1 82.819 Acc@5 96.668 loss 1.597\n",
      "Test ANN :  [190/196]  eta: 0:00:01  loss: 1.6380 (1.5973)  acc1: 78.5156 (82.8186)  acc5: 97.2656 (96.6684)  time: 0.3054  data: 0.0293  max mem: 2743\n",
      "192 : 80.078125 96.09375\n",
      "* Acc@1 82.804 Acc@5 96.665 loss 1.597\n",
      "193 : 86.328125 99.21875\n",
      "* Acc@1 82.823 Acc@5 96.679 loss 1.596\n",
      "194 : 96.484375 99.21875\n",
      "* Acc@1 82.893 Acc@5 96.692 loss 1.594\n",
      "195 : 90.625 98.4375\n",
      "* Acc@1 82.933 Acc@5 96.701 loss 1.593\n",
      "196 : 61.25 91.25\n",
      "* Acc@1 82.898 Acc@5 96.692 loss 1.595\n",
      "Test ANN :  [195/196]  eta: 0:00:00  loss: 1.5291 (1.5954)  acc1: 80.8594 (82.8980)  acc5: 97.2656 (96.6920)  time: 0.2992  data: 0.0299  max mem: 2743\n",
      "Test ANN : Total time: 0:01:02 (0.3207 s / it)\n",
      "* Acc@1 82.898 Acc@5 96.692 loss 1.595\n"
     ]
    }
   ],
   "source": [
    "!python run_class_finetuning.py --eval_data_path ../data/val --nb_classes 1000 --data_set image_folder --model vit_base_patch16_224 --model_path ../models/original_model/vit_base_patch16_224.pth --input_size 224 --batch_size 256 --test_mode ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af42868b-d58f-4062-ac9e-e9f5adb8a66d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:343: UserWarning: Overwriting vit_tiny_patch16_224 in registry with model_vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:353: UserWarning: Overwriting vit_tiny_patch16_384 in registry with model_vit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:386: UserWarning: Overwriting vit_small_patch32_224 in registry with model_vit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:398: UserWarning: Overwriting vit_small_patch32_384 in registry with model_vit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:410: UserWarning: Overwriting vit_small_patch16_224 in registry with model_vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:421: UserWarning: Overwriting vit_small_patch16_384 in registry with model_vit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:454: UserWarning: Overwriting vit_base_patch32_224 in registry with model_vit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained: bool = False, **kwargs) -> VisionTransformer:\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:465: UserWarning: Overwriting vit_base_patch32_384 in registry with model_vit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:476: UserWarning: Overwriting vit_base_patch16_224 in registry with model_vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:509: UserWarning: Overwriting vit_base_patch16_384 in registry with model_vit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:520: UserWarning: Overwriting vit_large_patch32_224 in registry with model_vit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:531: UserWarning: Overwriting vit_large_patch32_384 in registry with model_vit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:542: UserWarning: Overwriting vit_large_patch16_224 in registry with model_vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:553: UserWarning: Overwriting vit_large_patch16_384 in registry with model_vit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "Namespace(batch_size=256,\n",
      "data_set='image_folder',\n",
      "device='cuda',\n",
      "distributed=False,\n",
      "eval_data_path='../data/val',\n",
      "imagenet_default_mean_and_std=False,\n",
      "input_size=224,\n",
      "linear_num=8,\n",
      "model='vit_large_patch16_224',\n",
      "model_path='../models/original_model/vit_large_patch16_224.pth',\n",
      "monitor=True,\n",
      "nb_classes=1000,\n",
      "num_workers=10,\n",
      "output_dir='../models',\n",
      "percent=0.99,\n",
      "qkv_num=8,\n",
      "savename='test',\n",
      "seed=0,\n",
      "softmax_num=8,\n",
      "softmax_p=0.0125,\n",
      "test_T=8,\n",
      "test_mode='ann')\n",
      "Number of the class = 1000\n",
      " patchembed dropout sequential layernorm identity linear \n",
      "patch_embed conv2d identity \n",
      "blocks block block block block block block block block block block block block block block block block block block block block block block block block \n",
      "0 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "1 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "2 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "3 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "4 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "5 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "6 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "7 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "8 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "9 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "10 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "11 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "12 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "13 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "14 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "15 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "16 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "17 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "18 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "19 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "20 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "21 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "22 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "23 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "Load ckpt from ../models/original_model/vit_large_patch16_224.pth\n",
      "Load state_dict by model_key = model\n",
      "number of params: 304326632\n",
      "Batch size = 256\n",
      "1 : 94.921875 99.21875\n",
      "* Acc@1 94.922 Acc@5 99.219 loss 1.745\n",
      "Test ANN :  [  0/196]  eta: 0:17:58  loss: 1.7455 (1.7455)  acc1: 94.9219 (94.9219)  acc5: 99.2188 (99.2188)  time: 5.5025  data: 3.6140  max mem: 4623\n",
      "2 : 92.1875 98.046875\n",
      "* Acc@1 93.555 Acc@5 98.633 loss 1.767\n",
      "3 : 98.4375 99.609375\n",
      "* Acc@1 95.182 Acc@5 98.958 loss 1.722\n",
      "4 : 94.921875 99.609375\n",
      "* Acc@1 95.117 Acc@5 99.121 loss 1.713\n",
      "5 : 96.484375 100.0\n",
      "* Acc@1 95.391 Acc@5 99.297 loss 1.704\n",
      "6 : 84.375 98.046875\n",
      "* Acc@1 93.555 Acc@5 99.089 loss 1.754\n",
      "7 : 78.515625 97.265625\n",
      "* Acc@1 91.406 Acc@5 98.828 loss 1.797\n",
      "8 : 80.078125 98.828125\n",
      "* Acc@1 89.990 Acc@5 98.828 loss 1.818\n",
      "9 : 85.15625 96.484375\n",
      "* Acc@1 89.453 Acc@5 98.568 loss 1.840\n",
      "10 : 85.9375 98.4375\n",
      "* Acc@1 89.102 Acc@5 98.555 loss 1.849\n",
      "11 : 82.8125 96.484375\n",
      "* Acc@1 88.530 Acc@5 98.366 loss 1.872\n",
      "Test ANN :  [ 10/196]  eta: 0:03:50  loss: 1.9286 (1.8719)  acc1: 85.9375 (88.5298)  acc5: 98.4375 (98.3665)  time: 1.2390  data: 0.3287  max mem: 4623\n",
      "12 : 78.90625 96.484375\n",
      "* Acc@1 87.728 Acc@5 98.210 loss 1.894\n",
      "13 : 78.125 96.09375\n",
      "* Acc@1 86.989 Acc@5 98.047 loss 1.913\n",
      "14 : 83.203125 98.828125\n",
      "* Acc@1 86.719 Acc@5 98.103 loss 1.913\n",
      "15 : 80.078125 98.046875\n",
      "* Acc@1 86.276 Acc@5 98.099 loss 1.923\n",
      "16 : 82.8125 96.875\n",
      "* Acc@1 86.060 Acc@5 98.022 loss 1.935\n",
      "17 : 94.53125 99.21875\n",
      "* Acc@1 86.558 Acc@5 98.093 loss 1.923\n",
      "18 : 98.4375 99.609375\n",
      "* Acc@1 87.218 Acc@5 98.177 loss 1.906\n",
      "19 : 95.703125 99.21875\n",
      "* Acc@1 87.664 Acc@5 98.232 loss 1.895\n",
      "20 : 93.75 100.0\n",
      "* Acc@1 87.969 Acc@5 98.320 loss 1.884\n",
      "21 : 92.96875 97.265625\n",
      "* Acc@1 88.207 Acc@5 98.270 loss 1.881\n",
      "Test ANN :  [ 20/196]  eta: 0:03:02  loss: 1.9198 (1.8815)  acc1: 85.1562 (88.2068)  acc5: 98.0469 (98.2701)  time: 0.8150  data: 0.0002  max mem: 4623\n",
      "22 : 91.015625 98.828125\n",
      "* Acc@1 88.335 Acc@5 98.295 loss 1.879\n",
      "23 : 92.578125 98.4375\n",
      "* Acc@1 88.519 Acc@5 98.302 loss 1.875\n",
      "24 : 83.984375 98.4375\n",
      "* Acc@1 88.330 Acc@5 98.307 loss 1.877\n",
      "25 : 90.625 97.265625\n",
      "* Acc@1 88.422 Acc@5 98.266 loss 1.875\n",
      "26 : 96.09375 99.609375\n",
      "* Acc@1 88.717 Acc@5 98.317 loss 1.866\n",
      "27 : 93.75 98.828125\n",
      "* Acc@1 88.903 Acc@5 98.336 loss 1.863\n",
      "28 : 94.53125 99.609375\n",
      "* Acc@1 89.104 Acc@5 98.382 loss 1.858\n",
      "29 : 99.21875 100.0\n",
      "* Acc@1 89.453 Acc@5 98.438 loss 1.848\n",
      "30 : 89.453125 99.609375\n",
      "* Acc@1 89.453 Acc@5 98.477 loss 1.844\n",
      "31 : 85.9375 96.09375\n",
      "* Acc@1 89.340 Acc@5 98.400 loss 1.846\n",
      "Test ANN :  [ 30/196]  eta: 0:02:40  loss: 1.7880 (1.8457)  acc1: 91.0156 (89.3397)  acc5: 98.8281 (98.3997)  time: 0.8200  data: 0.0002  max mem: 4623\n",
      "32 : 84.375 98.046875\n",
      "* Acc@1 89.185 Acc@5 98.389 loss 1.849\n",
      "33 : 70.703125 93.75\n",
      "* Acc@1 88.625 Acc@5 98.248 loss 1.865\n",
      "34 : 83.203125 95.703125\n",
      "* Acc@1 88.465 Acc@5 98.173 loss 1.869\n",
      "35 : 87.5 97.65625\n",
      "* Acc@1 88.438 Acc@5 98.158 loss 1.868\n",
      "36 : 84.375 96.484375\n",
      "* Acc@1 88.325 Acc@5 98.112 loss 1.869\n",
      "37 : 78.515625 97.265625\n",
      "* Acc@1 88.060 Acc@5 98.089 loss 1.874\n",
      "38 : 82.03125 94.140625\n",
      "* Acc@1 87.901 Acc@5 97.985 loss 1.879\n",
      "39 : 83.203125 98.4375\n",
      "* Acc@1 87.780 Acc@5 97.997 loss 1.880\n",
      "40 : 79.6875 98.4375\n",
      "* Acc@1 87.578 Acc@5 98.008 loss 1.882\n",
      "41 : 88.671875 96.09375\n",
      "* Acc@1 87.605 Acc@5 97.961 loss 1.880\n",
      "Test ANN :  [ 40/196]  eta: 0:02:26  loss: 1.8325 (1.8805)  acc1: 85.9375 (87.6048)  acc5: 98.0469 (97.9611)  time: 0.8369  data: 0.0141  max mem: 4623\n",
      "42 : 86.71875 96.484375\n",
      "* Acc@1 87.584 Acc@5 97.926 loss 1.879\n",
      "43 : 89.0625 99.609375\n",
      "* Acc@1 87.618 Acc@5 97.965 loss 1.876\n",
      "44 : 83.984375 97.65625\n",
      "* Acc@1 87.536 Acc@5 97.958 loss 1.879\n",
      "45 : 81.640625 95.3125\n",
      "* Acc@1 87.405 Acc@5 97.899 loss 1.883\n",
      "46 : 80.859375 98.046875\n",
      "* Acc@1 87.262 Acc@5 97.903 loss 1.884\n",
      "47 : 78.90625 97.265625\n",
      "* Acc@1 87.084 Acc@5 97.889 loss 1.888\n",
      "48 : 80.859375 98.828125\n",
      "* Acc@1 86.955 Acc@5 97.909 loss 1.890\n",
      "49 : 77.34375 98.046875\n",
      "* Acc@1 86.759 Acc@5 97.911 loss 1.893\n",
      "50 : 94.53125 98.4375\n",
      "* Acc@1 86.914 Acc@5 97.922 loss 1.889\n",
      "51 : 90.625 98.4375\n",
      "* Acc@1 86.987 Acc@5 97.932 loss 1.886\n",
      "Test ANN :  [ 50/196]  eta: 0:02:14  loss: 1.9667 (1.8862)  acc1: 83.2031 (86.9868)  acc5: 97.6562 (97.9320)  time: 0.8528  data: 0.0276  max mem: 4623\n",
      "52 : 80.859375 97.265625\n",
      "* Acc@1 86.869 Acc@5 97.919 loss 1.887\n",
      "53 : 83.59375 98.046875\n",
      "* Acc@1 86.807 Acc@5 97.922 loss 1.888\n",
      "54 : 86.328125 96.09375\n",
      "* Acc@1 86.798 Acc@5 97.888 loss 1.894\n",
      "55 : 76.5625 97.265625\n",
      "* Acc@1 86.612 Acc@5 97.876 loss 1.898\n",
      "56 : 73.4375 97.265625\n",
      "* Acc@1 86.377 Acc@5 97.866 loss 1.902\n",
      "57 : 90.234375 99.609375\n",
      "* Acc@1 86.445 Acc@5 97.896 loss 1.901\n",
      "58 : 96.09375 98.828125\n",
      "* Acc@1 86.611 Acc@5 97.912 loss 1.897\n",
      "59 : 88.671875 97.265625\n",
      "* Acc@1 86.646 Acc@5 97.901 loss 1.898\n",
      "60 : 84.375 99.21875\n",
      "* Acc@1 86.608 Acc@5 97.923 loss 1.899\n",
      "61 : 83.203125 98.046875\n",
      "* Acc@1 86.552 Acc@5 97.925 loss 1.899\n",
      "Test ANN :  [ 60/196]  eta: 0:02:04  loss: 1.9277 (1.8990)  acc1: 83.5938 (86.5523)  acc5: 98.0469 (97.9252)  time: 0.8573  data: 0.0289  max mem: 4623\n",
      "62 : 87.890625 97.65625\n",
      "* Acc@1 86.574 Acc@5 97.921 loss 1.899\n",
      "63 : 93.359375 99.609375\n",
      "* Acc@1 86.682 Acc@5 97.948 loss 1.897\n",
      "64 : 97.265625 99.609375\n",
      "* Acc@1 86.847 Acc@5 97.974 loss 1.895\n",
      "65 : 88.671875 97.65625\n",
      "* Acc@1 86.875 Acc@5 97.969 loss 1.894\n",
      "66 : 92.96875 98.4375\n",
      "* Acc@1 86.967 Acc@5 97.976 loss 1.891\n",
      "67 : 88.671875 99.21875\n",
      "* Acc@1 86.993 Acc@5 97.994 loss 1.889\n",
      "68 : 85.9375 99.21875\n",
      "* Acc@1 86.977 Acc@5 98.012 loss 1.888\n",
      "69 : 89.84375 100.0\n",
      "* Acc@1 87.019 Acc@5 98.041 loss 1.886\n",
      "70 : 80.078125 99.609375\n",
      "* Acc@1 86.920 Acc@5 98.064 loss 1.888\n",
      "71 : 85.9375 98.4375\n",
      "* Acc@1 86.906 Acc@5 98.069 loss 1.888\n",
      "Test ANN :  [ 70/196]  eta: 0:01:54  loss: 1.8899 (1.8881)  acc1: 86.3281 (86.9058)  acc5: 98.4375 (98.0689)  time: 0.8589  data: 0.0230  max mem: 4623\n",
      "72 : 94.53125 97.65625\n",
      "* Acc@1 87.012 Acc@5 98.063 loss 1.886\n",
      "73 : 83.203125 98.046875\n",
      "* Acc@1 86.960 Acc@5 98.063 loss 1.889\n",
      "74 : 89.84375 96.09375\n",
      "* Acc@1 86.999 Acc@5 98.036 loss 1.890\n",
      "75 : 73.046875 96.09375\n",
      "* Acc@1 86.812 Acc@5 98.010 loss 1.896\n",
      "76 : 83.59375 99.21875\n",
      "* Acc@1 86.770 Acc@5 98.026 loss 1.896\n",
      "77 : 92.578125 99.21875\n",
      "* Acc@1 86.846 Acc@5 98.042 loss 1.895\n",
      "78 : 90.234375 96.875\n",
      "* Acc@1 86.889 Acc@5 98.027 loss 1.894\n",
      "79 : 83.59375 97.265625\n",
      "* Acc@1 86.847 Acc@5 98.017 loss 1.895\n",
      "80 : 83.984375 98.046875\n",
      "* Acc@1 86.812 Acc@5 98.018 loss 1.896\n",
      "81 : 81.25 97.65625\n",
      "* Acc@1 86.743 Acc@5 98.013 loss 1.897\n",
      "Test ANN :  [ 80/196]  eta: 0:01:44  loss: 1.8804 (1.8969)  acc1: 87.8906 (86.7429)  acc5: 98.0469 (98.0131)  time: 0.8585  data: 0.0206  max mem: 4623\n",
      "82 : 83.984375 96.09375\n",
      "* Acc@1 86.709 Acc@5 97.990 loss 1.899\n",
      "83 : 87.5 98.046875\n",
      "* Acc@1 86.719 Acc@5 97.990 loss 1.898\n",
      "84 : 88.28125 98.046875\n",
      "* Acc@1 86.737 Acc@5 97.991 loss 1.897\n",
      "85 : 86.71875 98.4375\n",
      "* Acc@1 86.737 Acc@5 97.996 loss 1.897\n",
      "86 : 80.078125 97.265625\n",
      "* Acc@1 86.660 Acc@5 97.988 loss 1.899\n",
      "87 : 80.46875 96.484375\n",
      "* Acc@1 86.589 Acc@5 97.971 loss 1.900\n",
      "88 : 87.5 97.265625\n",
      "* Acc@1 86.599 Acc@5 97.963 loss 1.899\n",
      "89 : 83.203125 96.09375\n",
      "* Acc@1 86.561 Acc@5 97.942 loss 1.901\n",
      "90 : 87.109375 97.265625\n",
      "* Acc@1 86.567 Acc@5 97.934 loss 1.901\n",
      "91 : 73.4375 95.3125\n",
      "* Acc@1 86.423 Acc@5 97.905 loss 1.905\n",
      "Test ANN :  [ 90/196]  eta: 0:01:34  loss: 1.9335 (1.9052)  acc1: 83.9844 (86.4226)  acc5: 97.2656 (97.9052)  time: 0.8609  data: 0.0270  max mem: 4623\n",
      "92 : 83.984375 96.09375\n",
      "* Acc@1 86.396 Acc@5 97.886 loss 1.906\n",
      "93 : 90.234375 98.4375\n",
      "* Acc@1 86.437 Acc@5 97.891 loss 1.904\n",
      "94 : 81.640625 96.875\n",
      "* Acc@1 86.386 Acc@5 97.881 loss 1.905\n",
      "95 : 66.015625 95.703125\n",
      "* Acc@1 86.172 Acc@5 97.858 loss 1.911\n",
      "96 : 76.953125 95.3125\n",
      "* Acc@1 86.076 Acc@5 97.831 loss 1.913\n",
      "97 : 80.859375 95.703125\n",
      "* Acc@1 86.022 Acc@5 97.809 loss 1.915\n",
      "98 : 81.25 94.140625\n",
      "* Acc@1 85.973 Acc@5 97.772 loss 1.917\n",
      "99 : 76.171875 96.484375\n",
      "* Acc@1 85.874 Acc@5 97.759 loss 1.920\n",
      "100 : 88.28125 97.65625\n",
      "* Acc@1 85.898 Acc@5 97.758 loss 1.919\n",
      "101 : 77.734375 98.046875\n",
      "* Acc@1 85.818 Acc@5 97.761 loss 1.919\n",
      "Test ANN :  [100/196]  eta: 0:01:25  loss: 2.0035 (1.9193)  acc1: 81.6406 (85.8176)  acc5: 96.4844 (97.7607)  time: 0.8628  data: 0.0277  max mem: 4623\n",
      "102 : 85.15625 98.828125\n",
      "* Acc@1 85.811 Acc@5 97.771 loss 1.919\n",
      "103 : 79.6875 98.4375\n",
      "* Acc@1 85.752 Acc@5 97.778 loss 1.920\n",
      "104 : 80.46875 96.484375\n",
      "* Acc@1 85.701 Acc@5 97.765 loss 1.922\n",
      "105 : 84.375 97.65625\n",
      "* Acc@1 85.688 Acc@5 97.764 loss 1.922\n",
      "106 : 82.8125 96.484375\n",
      "* Acc@1 85.661 Acc@5 97.752 loss 1.922\n",
      "107 : 85.15625 97.65625\n",
      "* Acc@1 85.656 Acc@5 97.751 loss 1.922\n",
      "108 : 85.15625 95.703125\n",
      "* Acc@1 85.652 Acc@5 97.732 loss 1.922\n",
      "109 : 88.671875 97.65625\n",
      "* Acc@1 85.679 Acc@5 97.732 loss 1.922\n",
      "110 : 87.890625 98.4375\n",
      "* Acc@1 85.700 Acc@5 97.738 loss 1.921\n",
      "111 : 87.109375 97.65625\n",
      "* Acc@1 85.712 Acc@5 97.737 loss 1.921\n",
      "Test ANN :  [110/196]  eta: 0:01:16  loss: 1.9364 (1.9210)  acc1: 82.8125 (85.7123)  acc5: 96.8750 (97.7372)  time: 0.8636  data: 0.0277  max mem: 4623\n",
      "112 : 91.796875 98.4375\n",
      "* Acc@1 85.767 Acc@5 97.743 loss 1.920\n",
      "113 : 92.96875 97.65625\n",
      "* Acc@1 85.830 Acc@5 97.743 loss 1.919\n",
      "114 : 81.640625 97.65625\n",
      "* Acc@1 85.794 Acc@5 97.742 loss 1.920\n",
      "115 : 81.25 96.484375\n",
      "* Acc@1 85.754 Acc@5 97.731 loss 1.920\n",
      "116 : 82.421875 96.484375\n",
      "* Acc@1 85.725 Acc@5 97.720 loss 1.921\n",
      "117 : 84.765625 96.484375\n",
      "* Acc@1 85.717 Acc@5 97.710 loss 1.921\n",
      "118 : 84.765625 94.140625\n",
      "* Acc@1 85.709 Acc@5 97.679 loss 1.922\n",
      "119 : 92.96875 98.046875\n",
      "* Acc@1 85.770 Acc@5 97.683 loss 1.921\n",
      "120 : 94.53125 98.046875\n",
      "* Acc@1 85.843 Acc@5 97.686 loss 1.919\n",
      "121 : 84.765625 95.703125\n",
      "* Acc@1 85.834 Acc@5 97.669 loss 1.920\n",
      "Test ANN :  [120/196]  eta: 0:01:07  loss: 1.9253 (1.9196)  acc1: 84.7656 (85.8342)  acc5: 97.6562 (97.6692)  time: 0.8642  data: 0.0277  max mem: 4623\n",
      "122 : 63.671875 91.40625\n",
      "* Acc@1 85.653 Acc@5 97.618 loss 1.924\n",
      "123 : 89.84375 98.046875\n",
      "* Acc@1 85.687 Acc@5 97.621 loss 1.923\n",
      "124 : 80.46875 94.140625\n",
      "* Acc@1 85.645 Acc@5 97.593 loss 1.925\n",
      "125 : 64.84375 98.828125\n",
      "* Acc@1 85.478 Acc@5 97.603 loss 1.926\n",
      "126 : 90.625 98.4375\n",
      "* Acc@1 85.519 Acc@5 97.610 loss 1.925\n",
      "127 : 92.96875 98.4375\n",
      "* Acc@1 85.578 Acc@5 97.616 loss 1.924\n",
      "128 : 81.640625 96.484375\n",
      "* Acc@1 85.547 Acc@5 97.607 loss 1.924\n",
      "129 : 66.796875 95.3125\n",
      "* Acc@1 85.402 Acc@5 97.590 loss 1.927\n",
      "130 : 69.921875 94.921875\n",
      "* Acc@1 85.282 Acc@5 97.569 loss 1.929\n",
      "131 : 91.40625 99.21875\n",
      "* Acc@1 85.329 Acc@5 97.582 loss 1.927\n",
      "Test ANN :  [130/196]  eta: 0:00:58  loss: 1.9586 (1.9274)  acc1: 84.7656 (85.3292)  acc5: 96.4844 (97.5817)  time: 0.8647  data: 0.0276  max mem: 4623\n",
      "132 : 85.9375 98.4375\n",
      "* Acc@1 85.334 Acc@5 97.588 loss 1.927\n",
      "133 : 85.546875 96.09375\n",
      "* Acc@1 85.335 Acc@5 97.577 loss 1.927\n",
      "134 : 79.6875 96.875\n",
      "* Acc@1 85.293 Acc@5 97.572 loss 1.929\n",
      "135 : 84.765625 97.265625\n",
      "* Acc@1 85.289 Acc@5 97.569 loss 1.929\n",
      "136 : 82.8125 96.875\n",
      "* Acc@1 85.271 Acc@5 97.564 loss 1.929\n",
      "137 : 85.9375 96.875\n",
      "* Acc@1 85.276 Acc@5 97.559 loss 1.929\n",
      "138 : 83.984375 98.046875\n",
      "* Acc@1 85.267 Acc@5 97.563 loss 1.930\n",
      "139 : 83.59375 97.65625\n",
      "* Acc@1 85.255 Acc@5 97.564 loss 1.929\n",
      "140 : 92.1875 98.4375\n",
      "* Acc@1 85.304 Acc@5 97.570 loss 1.928\n",
      "141 : 87.5 96.875\n",
      "* Acc@1 85.320 Acc@5 97.565 loss 1.928\n",
      "Test ANN :  [140/196]  eta: 0:00:49  loss: 1.9309 (1.9282)  acc1: 83.9844 (85.3197)  acc5: 96.8750 (97.5648)  time: 0.8653  data: 0.0279  max mem: 4623\n",
      "142 : 86.71875 96.875\n",
      "* Acc@1 85.330 Acc@5 97.560 loss 1.928\n",
      "143 : 81.25 93.359375\n",
      "* Acc@1 85.301 Acc@5 97.531 loss 1.929\n",
      "144 : 83.984375 97.265625\n",
      "* Acc@1 85.292 Acc@5 97.529 loss 1.929\n",
      "145 : 86.328125 96.484375\n",
      "* Acc@1 85.299 Acc@5 97.522 loss 1.929\n",
      "146 : 82.03125 98.046875\n",
      "* Acc@1 85.277 Acc@5 97.525 loss 1.929\n",
      "147 : 83.203125 95.703125\n",
      "* Acc@1 85.263 Acc@5 97.513 loss 1.930\n",
      "148 : 89.84375 97.65625\n",
      "* Acc@1 85.293 Acc@5 97.514 loss 1.929\n",
      "149 : 85.546875 95.703125\n",
      "* Acc@1 85.295 Acc@5 97.502 loss 1.930\n",
      "150 : 85.15625 94.921875\n",
      "* Acc@1 85.294 Acc@5 97.484 loss 1.930\n",
      "151 : 87.109375 98.046875\n",
      "* Acc@1 85.306 Acc@5 97.488 loss 1.929\n",
      "Test ANN :  [150/196]  eta: 0:00:40  loss: 1.9185 (1.9295)  acc1: 85.1562 (85.3063)  acc5: 96.8750 (97.4881)  time: 0.8656  data: 0.0277  max mem: 4623\n",
      "152 : 83.59375 96.484375\n",
      "* Acc@1 85.295 Acc@5 97.481 loss 1.930\n",
      "153 : 81.640625 98.046875\n",
      "* Acc@1 85.271 Acc@5 97.485 loss 1.930\n",
      "154 : 89.84375 97.265625\n",
      "* Acc@1 85.301 Acc@5 97.484 loss 1.929\n",
      "155 : 87.890625 96.875\n",
      "* Acc@1 85.318 Acc@5 97.480 loss 1.928\n",
      "156 : 84.375 96.484375\n",
      "* Acc@1 85.311 Acc@5 97.473 loss 1.929\n",
      "157 : 87.890625 97.265625\n",
      "* Acc@1 85.328 Acc@5 97.472 loss 1.928\n",
      "158 : 86.71875 98.4375\n",
      "* Acc@1 85.337 Acc@5 97.478 loss 1.928\n",
      "159 : 73.046875 97.265625\n",
      "* Acc@1 85.259 Acc@5 97.477 loss 1.930\n",
      "160 : 78.125 96.875\n",
      "* Acc@1 85.215 Acc@5 97.473 loss 1.931\n",
      "161 : 89.84375 98.046875\n",
      "* Acc@1 85.244 Acc@5 97.477 loss 1.930\n",
      "Test ANN :  [160/196]  eta: 0:00:31  loss: 1.9128 (1.9299)  acc1: 85.1562 (85.2436)  acc5: 96.8750 (97.4767)  time: 0.8656  data: 0.0275  max mem: 4623\n",
      "162 : 83.984375 96.09375\n",
      "* Acc@1 85.236 Acc@5 97.468 loss 1.930\n",
      "163 : 90.625 98.828125\n",
      "* Acc@1 85.269 Acc@5 97.477 loss 1.929\n",
      "164 : 63.28125 95.3125\n",
      "* Acc@1 85.135 Acc@5 97.463 loss 1.931\n",
      "165 : 85.15625 98.046875\n",
      "* Acc@1 85.135 Acc@5 97.467 loss 1.931\n",
      "166 : 76.5625 98.046875\n",
      "* Acc@1 85.083 Acc@5 97.470 loss 1.932\n",
      "167 : 88.28125 98.828125\n",
      "* Acc@1 85.102 Acc@5 97.478 loss 1.930\n",
      "168 : 80.46875 96.484375\n",
      "* Acc@1 85.075 Acc@5 97.473 loss 1.932\n",
      "169 : 84.375 96.875\n",
      "* Acc@1 85.071 Acc@5 97.469 loss 1.931\n",
      "170 : 83.59375 96.09375\n",
      "* Acc@1 85.062 Acc@5 97.461 loss 1.931\n",
      "171 : 92.1875 99.21875\n",
      "* Acc@1 85.104 Acc@5 97.471 loss 1.930\n",
      "Test ANN :  [170/196]  eta: 0:00:22  loss: 1.9128 (1.9302)  acc1: 84.3750 (85.1037)  acc5: 97.2656 (97.4712)  time: 0.8648  data: 0.0264  max mem: 4623\n",
      "172 : 81.640625 97.265625\n",
      "* Acc@1 85.084 Acc@5 97.470 loss 1.930\n",
      "173 : 78.515625 94.53125\n",
      "* Acc@1 85.046 Acc@5 97.453 loss 1.932\n",
      "174 : 85.15625 95.703125\n",
      "* Acc@1 85.046 Acc@5 97.443 loss 1.932\n",
      "175 : 84.765625 98.046875\n",
      "* Acc@1 85.045 Acc@5 97.446 loss 1.932\n",
      "176 : 78.125 95.3125\n",
      "* Acc@1 85.005 Acc@5 97.434 loss 1.933\n",
      "177 : 84.765625 95.703125\n",
      "* Acc@1 85.004 Acc@5 97.425 loss 1.934\n",
      "178 : 64.84375 97.265625\n",
      "* Acc@1 84.891 Acc@5 97.424 loss 1.936\n",
      "179 : 87.5 98.4375\n",
      "* Acc@1 84.905 Acc@5 97.429 loss 1.935\n",
      "180 : 87.109375 97.65625\n",
      "* Acc@1 84.918 Acc@5 97.431 loss 1.935\n",
      "181 : 80.859375 98.4375\n",
      "* Acc@1 84.895 Acc@5 97.436 loss 1.935\n",
      "Test ANN :  [180/196]  eta: 0:00:14  loss: 1.9261 (1.9352)  acc1: 83.9844 (84.8951)  acc5: 97.2656 (97.4361)  time: 0.8636  data: 0.0249  max mem: 4623\n",
      "182 : 89.0625 97.65625\n",
      "* Acc@1 84.918 Acc@5 97.437 loss 1.934\n",
      "183 : 87.890625 97.65625\n",
      "* Acc@1 84.934 Acc@5 97.439 loss 1.934\n",
      "184 : 90.234375 99.21875\n",
      "* Acc@1 84.963 Acc@5 97.448 loss 1.933\n",
      "185 : 86.328125 98.828125\n",
      "* Acc@1 84.970 Acc@5 97.456 loss 1.933\n",
      "186 : 76.5625 98.046875\n",
      "* Acc@1 84.925 Acc@5 97.459 loss 1.934\n",
      "187 : 96.09375 99.609375\n",
      "* Acc@1 84.985 Acc@5 97.470 loss 1.933\n",
      "188 : 78.125 94.53125\n",
      "* Acc@1 84.948 Acc@5 97.455 loss 1.934\n",
      "189 : 82.03125 95.3125\n",
      "* Acc@1 84.933 Acc@5 97.443 loss 1.934\n",
      "190 : 69.921875 96.09375\n",
      "* Acc@1 84.854 Acc@5 97.436 loss 1.936\n",
      "191 : 76.5625 96.875\n",
      "* Acc@1 84.811 Acc@5 97.433 loss 1.937\n",
      "Test ANN :  [190/196]  eta: 0:00:05  loss: 1.9773 (1.9374)  acc1: 82.0312 (84.8106)  acc5: 97.2656 (97.4333)  time: 0.8632  data: 0.0243  max mem: 4623\n",
      "192 : 83.59375 96.484375\n",
      "* Acc@1 84.804 Acc@5 97.428 loss 1.937\n",
      "193 : 87.890625 99.21875\n",
      "* Acc@1 84.820 Acc@5 97.438 loss 1.937\n",
      "194 : 95.703125 98.828125\n",
      "* Acc@1 84.876 Acc@5 97.445 loss 1.936\n",
      "195 : 91.796875 98.828125\n",
      "* Acc@1 84.912 Acc@5 97.452 loss 1.935\n",
      "196 : 66.25 92.5\n",
      "* Acc@1 84.882 Acc@5 97.444 loss 1.938\n",
      "Test ANN :  [195/196]  eta: 0:00:00  loss: 1.9205 (1.9376)  acc1: 84.7656 (84.8820)  acc5: 97.6562 (97.4440)  time: 0.8389  data: 0.0249  max mem: 4623\n",
      "Test ANN : Total time: 0:02:51 (0.8768 s / it)\n",
      "* Acc@1 84.882 Acc@5 97.444 loss 1.938\n"
     ]
    }
   ],
   "source": [
    "!python run_class_finetuning.py --eval_data_path ../data/val --nb_classes 1000 --data_set image_folder --model vit_large_patch16_224 --model_path ../models/original_model/vit_large_patch16_224.pth --input_size 224 --batch_size 256 --test_mode ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14c557-2e22-4505-9df9-0e9361123dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:359: UserWarning: Overwriting vit_tiny_patch16_224 in registry with model_vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:369: UserWarning: Overwriting vit_tiny_patch16_384 in registry with model_vit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:402: UserWarning: Overwriting vit_small_patch32_224 in registry with model_vit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:414: UserWarning: Overwriting vit_small_patch32_384 in registry with model_vit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:426: UserWarning: Overwriting vit_small_patch16_224 in registry with model_vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:437: UserWarning: Overwriting vit_small_patch16_384 in registry with model_vit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:470: UserWarning: Overwriting vit_base_patch32_224 in registry with model_vit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained: bool = False, **kwargs) -> VisionTransformer:\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:481: UserWarning: Overwriting vit_base_patch32_384 in registry with model_vit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:492: UserWarning: Overwriting vit_base_patch16_224 in registry with model_vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:525: UserWarning: Overwriting vit_base_patch16_384 in registry with model_vit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:536: UserWarning: Overwriting vit_large_patch32_224 in registry with model_vit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:547: UserWarning: Overwriting vit_large_patch32_384 in registry with model_vit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:558: UserWarning: Overwriting vit_large_patch16_224 in registry with model_vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:569: UserWarning: Overwriting vit_large_patch16_384 in registry with model_vit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "Namespace(batch_size=128,\n",
      "data_set='image_folder',\n",
      "device='cuda',\n",
      "distributed=False,\n",
      "eval_data_path='../data/val',\n",
      "imagenet_default_mean_and_std=False,\n",
      "input_size=336,\n",
      "linear_num=8,\n",
      "model='eva_g_patch14',\n",
      "model_path='../models/original_model/eva_giant_patch14_336.pth',\n",
      "monitor=True,\n",
      "nb_classes=1000,\n",
      "num_workers=10,\n",
      "output_dir='../models',\n",
      "percent=0.99,\n",
      "qkv_num=8,\n",
      "savename='test',\n",
      "seed=0,\n",
      "softmax_num=8,\n",
      "softmax_p=0.0125,\n",
      "test_T=8,\n",
      "test_mode='ann')\n",
      "Number of the class = 1000\n",
      " patchembed dropout sequential identity layernorm identity linear \n",
      "patch_embed conv2d identity \n",
      "blocks block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block \n",
      "0 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "1 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "2 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "3 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "4 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "5 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "6 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "7 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "8 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "9 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "10 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "11 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "12 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "13 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "14 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "15 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "16 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "17 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "18 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "19 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "20 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "21 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "22 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "23 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "24 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "25 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "26 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "27 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "28 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "29 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "30 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "31 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "32 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "33 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "34 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "35 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "36 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "37 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "38 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "39 layernorm attention identity layernorm mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "Load ckpt from ../models/original_model/eva_giant_patch14_336.pth\n",
      "Load state_dict by model_key = model\n",
      "Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.qkv.bias', 'blocks.1.attn.qkv.bias', 'blocks.2.attn.qkv.bias', 'blocks.3.attn.qkv.bias', 'blocks.4.attn.qkv.bias', 'blocks.5.attn.qkv.bias', 'blocks.6.attn.qkv.bias', 'blocks.7.attn.qkv.bias', 'blocks.8.attn.qkv.bias', 'blocks.9.attn.qkv.bias', 'blocks.10.attn.qkv.bias', 'blocks.11.attn.qkv.bias', 'blocks.12.attn.qkv.bias', 'blocks.13.attn.qkv.bias', 'blocks.14.attn.qkv.bias', 'blocks.15.attn.qkv.bias', 'blocks.16.attn.qkv.bias', 'blocks.17.attn.qkv.bias', 'blocks.18.attn.qkv.bias', 'blocks.19.attn.qkv.bias', 'blocks.20.attn.qkv.bias', 'blocks.21.attn.qkv.bias', 'blocks.22.attn.qkv.bias', 'blocks.23.attn.qkv.bias', 'blocks.24.attn.qkv.bias', 'blocks.25.attn.qkv.bias', 'blocks.26.attn.qkv.bias', 'blocks.27.attn.qkv.bias', 'blocks.28.attn.qkv.bias', 'blocks.29.attn.qkv.bias', 'blocks.30.attn.qkv.bias', 'blocks.31.attn.qkv.bias', 'blocks.32.attn.qkv.bias', 'blocks.33.attn.qkv.bias', 'blocks.34.attn.qkv.bias', 'blocks.35.attn.qkv.bias', 'blocks.36.attn.qkv.bias', 'blocks.37.attn.qkv.bias', 'blocks.38.attn.qkv.bias', 'blocks.39.attn.qkv.bias']\n",
      "number of params: 1013174632\n",
      "Batch size = 128\n",
      "1 : 97.65625 99.21875\n",
      "* Acc@1 97.656 Acc@5 99.219 loss 1.424\n",
      "Test ANN :  [  0/391]  eta: 0:47:37  loss: 1.4243 (1.4243)  acc1: 97.6562 (97.6562)  acc5: 99.2188 (99.2188)  time: 7.3093  data: 1.8704  max mem: 14158\n",
      "2 : 95.3125 100.0\n",
      "* Acc@1 96.484 Acc@5 99.609 loss 1.465\n",
      "3 : 94.53125 99.21875\n",
      "* Acc@1 95.833 Acc@5 99.479 loss 1.486\n",
      "4 : 92.1875 99.21875\n",
      "* Acc@1 94.922 Acc@5 99.414 loss 1.495\n",
      "5 : 100.0 100.0\n",
      "* Acc@1 95.938 Acc@5 99.531 loss 1.461\n",
      "6 : 99.21875 100.0\n",
      "* Acc@1 96.484 Acc@5 99.609 loss 1.442\n",
      "7 : 97.65625 100.0\n",
      "* Acc@1 96.652 Acc@5 99.665 loss 1.432\n",
      "8 : 99.21875 100.0\n",
      "* Acc@1 96.973 Acc@5 99.707 loss 1.425\n",
      "9 : 97.65625 100.0\n",
      "* Acc@1 97.049 Acc@5 99.740 loss 1.431\n",
      "10 : 99.21875 100.0\n",
      "* Acc@1 97.266 Acc@5 99.766 loss 1.425\n",
      "11 : 80.46875 98.4375\n",
      "* Acc@1 95.739 Acc@5 99.645 loss 1.467\n",
      "Test ANN :  [ 10/391]  eta: 0:27:34  loss: 1.4243 (1.4673)  acc1: 97.6562 (95.7386)  acc5: 100.0000 (99.6449)  time: 4.3438  data: 0.1702  max mem: 14158\n",
      "12 : 93.75 98.4375\n",
      "* Acc@1 95.573 Acc@5 99.544 loss 1.468\n",
      "13 : 84.375 99.21875\n",
      "* Acc@1 94.712 Acc@5 99.519 loss 1.495\n",
      "14 : 85.15625 100.0\n",
      "* Acc@1 94.029 Acc@5 99.554 loss 1.513\n",
      "15 : 78.125 100.0\n",
      "* Acc@1 92.969 Acc@5 99.583 loss 1.536\n",
      "16 : 88.28125 100.0\n",
      "* Acc@1 92.676 Acc@5 99.609 loss 1.540\n",
      "17 : 92.1875 99.21875\n",
      "* Acc@1 92.647 Acc@5 99.586 loss 1.541\n",
      "18 : 87.5 96.875\n",
      "* Acc@1 92.361 Acc@5 99.436 loss 1.553\n",
      "19 : 85.15625 96.875\n",
      "* Acc@1 91.982 Acc@5 99.301 loss 1.560\n",
      "20 : 96.875 99.21875\n",
      "* Acc@1 92.227 Acc@5 99.297 loss 1.553\n",
      "21 : 94.53125 100.0\n",
      "* Acc@1 92.336 Acc@5 99.330 loss 1.549\n",
      "Test ANN :  [ 20/391]  eta: 0:26:04  loss: 1.5065 (1.5488)  acc1: 93.7500 (92.3363)  acc5: 100.0000 (99.3304)  time: 4.0629  data: 0.0002  max mem: 14158\n",
      "22 : 75.0 95.3125\n",
      "* Acc@1 91.548 Acc@5 99.148 loss 1.572\n",
      "23 : 95.3125 100.0\n",
      "* Acc@1 91.712 Acc@5 99.185 loss 1.569\n",
      "24 : 74.21875 98.4375\n",
      "* Acc@1 90.983 Acc@5 99.154 loss 1.589\n",
      "25 : 82.8125 98.4375\n",
      "* Acc@1 90.656 Acc@5 99.125 loss 1.598\n",
      "26 : 83.59375 97.65625\n",
      "* Acc@1 90.385 Acc@5 99.069 loss 1.606\n",
      "27 : 72.65625 99.21875\n",
      "* Acc@1 89.728 Acc@5 99.074 loss 1.616\n",
      "28 : 99.21875 100.0\n",
      "* Acc@1 90.067 Acc@5 99.107 loss 1.607\n",
      "29 : 78.90625 100.0\n",
      "* Acc@1 89.682 Acc@5 99.138 loss 1.617\n",
      "30 : 85.9375 98.4375\n",
      "* Acc@1 89.557 Acc@5 99.115 loss 1.619\n",
      "31 : 79.6875 92.96875\n",
      "* Acc@1 89.239 Acc@5 98.916 loss 1.632\n",
      "Test ANN :  [ 30/391]  eta: 0:25:08  loss: 1.7454 (1.6324)  acc1: 85.1562 (89.2389)  acc5: 99.2188 (98.9163)  time: 4.0885  data: 0.0002  max mem: 14158\n",
      "32 : 92.1875 100.0\n",
      "* Acc@1 89.331 Acc@5 98.950 loss 1.631\n",
      "33 : 97.65625 100.0\n",
      "* Acc@1 89.583 Acc@5 98.982 loss 1.625\n",
      "34 : 93.75 98.4375\n",
      "* Acc@1 89.706 Acc@5 98.966 loss 1.626\n",
      "35 : 100.0 100.0\n",
      "* Acc@1 90.000 Acc@5 98.996 loss 1.617\n",
      "36 : 99.21875 99.21875\n",
      "* Acc@1 90.256 Acc@5 99.002 loss 1.610\n",
      "37 : 98.4375 99.21875\n",
      "* Acc@1 90.477 Acc@5 99.008 loss 1.605\n",
      "38 : 97.65625 99.21875\n",
      "* Acc@1 90.666 Acc@5 99.013 loss 1.600\n",
      "39 : 98.4375 100.0\n",
      "* Acc@1 90.865 Acc@5 99.038 loss 1.595\n",
      "40 : 93.75 100.0\n",
      "* Acc@1 90.938 Acc@5 99.062 loss 1.593\n",
      "41 : 98.4375 100.0\n",
      "* Acc@1 91.120 Acc@5 99.085 loss 1.588\n",
      "Test ANN :  [ 40/391]  eta: 0:24:20  loss: 1.4964 (1.5879)  acc1: 93.7500 (91.1204)  acc5: 99.2188 (99.0854)  time: 4.1017  data: 0.0002  max mem: 14158\n",
      "42 : 95.3125 100.0\n",
      "* Acc@1 91.220 Acc@5 99.107 loss 1.586\n",
      "43 : 88.28125 99.21875\n",
      "* Acc@1 91.152 Acc@5 99.110 loss 1.587\n",
      "44 : 94.53125 98.4375\n",
      "* Acc@1 91.229 Acc@5 99.094 loss 1.586\n",
      "45 : 95.3125 100.0\n",
      "* Acc@1 91.319 Acc@5 99.115 loss 1.584\n",
      "46 : 97.65625 99.21875\n",
      "* Acc@1 91.457 Acc@5 99.117 loss 1.581\n",
      "47 : 84.375 100.0\n",
      "* Acc@1 91.307 Acc@5 99.136 loss 1.584\n",
      "48 : 89.84375 100.0\n",
      "* Acc@1 91.276 Acc@5 99.154 loss 1.585\n",
      "49 : 92.96875 100.0\n",
      "* Acc@1 91.311 Acc@5 99.171 loss 1.585\n",
      "50 : 96.875 100.0\n",
      "* Acc@1 91.422 Acc@5 99.188 loss 1.581\n",
      "51 : 99.21875 100.0\n",
      "* Acc@1 91.575 Acc@5 99.203 loss 1.577\n",
      "Test ANN :  [ 50/391]  eta: 0:23:35  loss: 1.4379 (1.5773)  acc1: 95.3125 (91.5748)  acc5: 100.0000 (99.2034)  time: 4.1058  data: 0.0003  max mem: 14158\n",
      "52 : 95.3125 100.0\n",
      "* Acc@1 91.647 Acc@5 99.219 loss 1.575\n",
      "53 : 92.96875 100.0\n",
      "* Acc@1 91.672 Acc@5 99.233 loss 1.573\n",
      "54 : 96.875 100.0\n",
      "* Acc@1 91.768 Acc@5 99.248 loss 1.570\n",
      "55 : 97.65625 100.0\n",
      "* Acc@1 91.875 Acc@5 99.261 loss 1.566\n",
      "56 : 96.875 100.0\n",
      "* Acc@1 91.964 Acc@5 99.275 loss 1.564\n",
      "57 : 100.0 100.0\n",
      "* Acc@1 92.105 Acc@5 99.287 loss 1.559\n",
      "58 : 100.0 100.0\n",
      "* Acc@1 92.241 Acc@5 99.300 loss 1.555\n",
      "59 : 98.4375 98.4375\n",
      "* Acc@1 92.346 Acc@5 99.285 loss 1.553\n",
      "60 : 91.40625 100.0\n",
      "* Acc@1 92.331 Acc@5 99.297 loss 1.554\n",
      "61 : 87.5 96.875\n",
      "* Acc@1 92.252 Acc@5 99.257 loss 1.557\n",
      "Test ANN :  [ 60/391]  eta: 0:22:51  loss: 1.4581 (1.5566)  acc1: 95.3125 (92.2515)  acc5: 100.0000 (99.2572)  time: 4.1077  data: 0.0003  max mem: 14158\n",
      "62 : 90.625 98.4375\n",
      "* Acc@1 92.225 Acc@5 99.244 loss 1.557\n",
      "63 : 95.3125 100.0\n",
      "* Acc@1 92.274 Acc@5 99.256 loss 1.555\n",
      "64 : 82.8125 97.65625\n",
      "* Acc@1 92.126 Acc@5 99.231 loss 1.561\n",
      "65 : 87.5 96.09375\n",
      "* Acc@1 92.055 Acc@5 99.183 loss 1.563\n",
      "66 : 71.09375 94.53125\n",
      "* Acc@1 91.738 Acc@5 99.112 loss 1.571\n",
      "67 : 93.75 96.875\n",
      "* Acc@1 91.768 Acc@5 99.079 loss 1.572\n",
      "68 : 88.28125 99.21875\n",
      "* Acc@1 91.716 Acc@5 99.081 loss 1.573\n",
      "69 : 90.625 98.4375\n",
      "* Acc@1 91.701 Acc@5 99.072 loss 1.574\n",
      "70 : 96.09375 100.0\n",
      "* Acc@1 91.763 Acc@5 99.085 loss 1.571\n",
      "71 : 89.84375 98.4375\n",
      "* Acc@1 91.736 Acc@5 99.076 loss 1.572\n",
      "Test ANN :  [ 70/391]  eta: 0:22:08  loss: 1.4709 (1.5720)  acc1: 92.9688 (91.7364)  acc5: 99.2188 (99.0757)  time: 4.1090  data: 0.0003  max mem: 14158\n",
      "72 : 92.96875 96.875\n",
      "* Acc@1 91.753 Acc@5 99.045 loss 1.572\n",
      "73 : 86.71875 98.4375\n",
      "* Acc@1 91.685 Acc@5 99.037 loss 1.573\n",
      "74 : 85.15625 99.21875\n",
      "* Acc@1 91.596 Acc@5 99.039 loss 1.574\n",
      "75 : 92.1875 96.09375\n",
      "* Acc@1 91.604 Acc@5 99.000 loss 1.575\n",
      "76 : 80.46875 95.3125\n",
      "* Acc@1 91.458 Acc@5 98.951 loss 1.578\n",
      "77 : 92.1875 98.4375\n",
      "* Acc@1 91.467 Acc@5 98.945 loss 1.578\n",
      "78 : 91.40625 99.21875\n",
      "* Acc@1 91.466 Acc@5 98.948 loss 1.578\n",
      "79 : 86.71875 99.21875\n",
      "* Acc@1 91.406 Acc@5 98.952 loss 1.580\n",
      "80 : 92.96875 100.0\n",
      "* Acc@1 91.426 Acc@5 98.965 loss 1.579\n",
      "81 : 90.625 97.65625\n",
      "* Acc@1 91.416 Acc@5 98.949 loss 1.579\n",
      "Test ANN :  [ 80/391]  eta: 0:21:26  loss: 1.6142 (1.5785)  acc1: 90.6250 (91.4159)  acc5: 98.4375 (98.9487)  time: 4.1095  data: 0.0003  max mem: 14158\n",
      "82 : 94.53125 98.4375\n",
      "* Acc@1 91.454 Acc@5 98.942 loss 1.577\n",
      "83 : 89.84375 98.4375\n",
      "* Acc@1 91.434 Acc@5 98.936 loss 1.577\n",
      "84 : 94.53125 98.4375\n",
      "* Acc@1 91.471 Acc@5 98.930 loss 1.577\n",
      "85 : 96.09375 100.0\n",
      "* Acc@1 91.526 Acc@5 98.943 loss 1.575\n",
      "86 : 95.3125 100.0\n",
      "* Acc@1 91.570 Acc@5 98.955 loss 1.574\n",
      "87 : 87.5 97.65625\n",
      "* Acc@1 91.523 Acc@5 98.940 loss 1.576\n",
      "88 : 92.1875 97.65625\n",
      "* Acc@1 91.531 Acc@5 98.926 loss 1.576\n",
      "89 : 83.59375 96.875\n",
      "* Acc@1 91.441 Acc@5 98.903 loss 1.580\n",
      "90 : 96.875 98.4375\n",
      "* Acc@1 91.502 Acc@5 98.898 loss 1.578\n",
      "91 : 81.25 100.0\n",
      "* Acc@1 91.389 Acc@5 98.910 loss 1.581\n",
      "Test ANN :  [ 90/391]  eta: 0:20:43  loss: 1.5747 (1.5806)  acc1: 91.4062 (91.3891)  acc5: 98.4375 (98.9097)  time: 4.1094  data: 0.0003  max mem: 14158\n",
      "92 : 94.53125 99.21875\n",
      "* Acc@1 91.423 Acc@5 98.913 loss 1.580\n",
      "93 : 89.84375 100.0\n",
      "* Acc@1 91.406 Acc@5 98.925 loss 1.580\n",
      "94 : 82.8125 99.21875\n",
      "* Acc@1 91.315 Acc@5 98.928 loss 1.581\n",
      "95 : 82.03125 100.0\n",
      "* Acc@1 91.217 Acc@5 98.939 loss 1.582\n",
      "96 : 91.40625 100.0\n",
      "* Acc@1 91.219 Acc@5 98.950 loss 1.581\n",
      "97 : 85.9375 97.65625\n",
      "* Acc@1 91.165 Acc@5 98.937 loss 1.582\n",
      "98 : 75.0 98.4375\n",
      "* Acc@1 91.000 Acc@5 98.932 loss 1.584\n",
      "99 : 95.3125 97.65625\n",
      "* Acc@1 91.043 Acc@5 98.919 loss 1.584\n",
      "100 : 98.4375 100.0\n",
      "* Acc@1 91.117 Acc@5 98.930 loss 1.581\n",
      "101 : 95.3125 97.65625\n",
      "* Acc@1 91.159 Acc@5 98.917 loss 1.580\n",
      "Test ANN :  [100/391]  eta: 0:20:01  loss: 1.5097 (1.5803)  acc1: 91.4062 (91.1587)  acc5: 98.4375 (98.9171)  time: 4.1097  data: 0.0003  max mem: 14158\n",
      "102 : 96.875 99.21875\n",
      "* Acc@1 91.215 Acc@5 98.920 loss 1.579\n",
      "103 : 98.4375 99.21875\n",
      "* Acc@1 91.285 Acc@5 98.923 loss 1.577\n",
      "104 : 82.8125 100.0\n",
      "* Acc@1 91.203 Acc@5 98.933 loss 1.577\n",
      "105 : 91.40625 100.0\n",
      "* Acc@1 91.205 Acc@5 98.943 loss 1.577\n",
      "106 : 87.5 96.875\n",
      "* Acc@1 91.170 Acc@5 98.924 loss 1.578\n",
      "107 : 74.21875 94.53125\n",
      "* Acc@1 91.012 Acc@5 98.883 loss 1.584\n",
      "108 : 96.875 99.21875\n",
      "* Acc@1 91.066 Acc@5 98.886 loss 1.583\n",
      "109 : 75.78125 97.65625\n",
      "* Acc@1 90.926 Acc@5 98.875 loss 1.586\n",
      "110 : 85.15625 99.21875\n",
      "* Acc@1 90.874 Acc@5 98.878 loss 1.587\n",
      "111 : 64.84375 100.0\n",
      "* Acc@1 90.639 Acc@5 98.888 loss 1.590\n",
      "Test ANN :  [110/391]  eta: 0:19:20  loss: 1.5875 (1.5903)  acc1: 87.5000 (90.6391)  acc5: 99.2188 (98.8880)  time: 4.1099  data: 0.0003  max mem: 14158\n",
      "112 : 86.71875 97.65625\n",
      "* Acc@1 90.604 Acc@5 98.877 loss 1.592\n",
      "113 : 88.28125 97.65625\n",
      "* Acc@1 90.584 Acc@5 98.866 loss 1.594\n",
      "114 : 92.1875 99.21875\n",
      "* Acc@1 90.598 Acc@5 98.869 loss 1.593\n",
      "115 : 96.09375 98.4375\n",
      "* Acc@1 90.645 Acc@5 98.865 loss 1.593\n",
      "116 : 96.875 100.0\n",
      "* Acc@1 90.699 Acc@5 98.875 loss 1.591\n",
      "117 : 85.15625 96.875\n",
      "* Acc@1 90.652 Acc@5 98.858 loss 1.593\n",
      "118 : 93.75 100.0\n",
      "* Acc@1 90.678 Acc@5 98.868 loss 1.592\n",
      "119 : 79.6875 99.21875\n",
      "* Acc@1 90.586 Acc@5 98.871 loss 1.594\n",
      "120 : 93.75 98.4375\n",
      "* Acc@1 90.612 Acc@5 98.867 loss 1.594\n",
      "121 : 91.40625 100.0\n",
      "* Acc@1 90.619 Acc@5 98.877 loss 1.594\n",
      "Test ANN :  [120/391]  eta: 0:18:38  loss: 1.5732 (1.5938)  acc1: 88.2812 (90.6185)  acc5: 99.2188 (98.8765)  time: 4.1086  data: 0.0003  max mem: 14158\n",
      "122 : 82.03125 97.65625\n",
      "* Acc@1 90.548 Acc@5 98.867 loss 1.596\n",
      "123 : 85.15625 98.4375\n",
      "* Acc@1 90.504 Acc@5 98.863 loss 1.596\n",
      "124 : 98.4375 99.21875\n",
      "* Acc@1 90.568 Acc@5 98.866 loss 1.595\n",
      "125 : 90.625 98.4375\n",
      "* Acc@1 90.569 Acc@5 98.862 loss 1.595\n",
      "126 : 96.875 100.0\n",
      "* Acc@1 90.619 Acc@5 98.872 loss 1.594\n",
      "127 : 98.4375 100.0\n",
      "* Acc@1 90.680 Acc@5 98.880 loss 1.592\n",
      "128 : 98.4375 100.0\n",
      "* Acc@1 90.741 Acc@5 98.889 loss 1.591\n",
      "129 : 93.75 100.0\n",
      "* Acc@1 90.764 Acc@5 98.898 loss 1.590\n",
      "130 : 92.96875 98.4375\n",
      "* Acc@1 90.781 Acc@5 98.894 loss 1.590\n",
      "131 : 97.65625 100.0\n",
      "* Acc@1 90.834 Acc@5 98.903 loss 1.588\n",
      "Test ANN :  [130/391]  eta: 0:17:56  loss: 1.5447 (1.5880)  acc1: 92.9688 (90.8337)  acc5: 99.2188 (98.9027)  time: 4.1086  data: 0.0003  max mem: 14158\n",
      "132 : 94.53125 99.21875\n",
      "* Acc@1 90.862 Acc@5 98.905 loss 1.588\n",
      "133 : 98.4375 100.0\n",
      "* Acc@1 90.919 Acc@5 98.913 loss 1.586\n",
      "134 : 83.59375 100.0\n",
      "* Acc@1 90.864 Acc@5 98.921 loss 1.586\n",
      "135 : 93.75 100.0\n",
      "* Acc@1 90.885 Acc@5 98.929 loss 1.586\n",
      "136 : 89.0625 99.21875\n",
      "* Acc@1 90.872 Acc@5 98.932 loss 1.585\n"
     ]
    }
   ],
   "source": [
    "!python run_class_finetuning.py --eval_data_path ../data/val --nb_classes 1000 --data_set image_folder --model eva_g_patch14 --model_path ../models/original_model/eva_giant_patch14_336.pth --input_size 336 --batch_size 128 --test_mode ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c304a-e2d3-4082-9b2a-a4c1f0569cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
