{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9d7f54d-a11b-4d70-9c12-3906f24b4adc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:359: UserWarning: Overwriting vit_tiny_patch16_224 in registry with model_vit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:369: UserWarning: Overwriting vit_tiny_patch16_384 in registry with model_vit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_tiny_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:402: UserWarning: Overwriting vit_small_patch32_224 in registry with model_vit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:414: UserWarning: Overwriting vit_small_patch32_384 in registry with model_vit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:426: UserWarning: Overwriting vit_small_patch16_224 in registry with model_vit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:437: UserWarning: Overwriting vit_small_patch16_384 in registry with model_vit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:470: UserWarning: Overwriting vit_base_patch32_224 in registry with model_vit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_224(pretrained: bool = False, **kwargs) -> VisionTransformer:\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:481: UserWarning: Overwriting vit_base_patch32_384 in registry with model_vit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:492: UserWarning: Overwriting vit_base_patch16_224 in registry with model_vit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:525: UserWarning: Overwriting vit_base_patch16_384 in registry with model_vit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:536: UserWarning: Overwriting vit_large_patch32_224 in registry with model_vit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:547: UserWarning: Overwriting vit_large_patch32_384 in registry with model_vit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch32_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:558: UserWarning: Overwriting vit_large_patch16_224 in registry with model_vit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "/root/autodl-tmp/Transformer-to-SNN-ECMT/model_vit.py:569: UserWarning: Overwriting vit_large_patch16_384 in registry with model_vit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained = False,num_classes: int = 1000,**keywords):\n",
      "Namespace(batch_size=128,\n",
      "data_set='image_folder',\n",
      "device='cuda',\n",
      "distributed=False,\n",
      "eval_data_path='../data/val',\n",
      "imagenet_default_mean_and_std=False,\n",
      "input_size=336,\n",
      "linear_num=8,\n",
      "model='eva_g_patch14',\n",
      "model_path='../models/original_model/eva_giant_patch14_336.pth',\n",
      "monitor=True,\n",
      "nb_classes=1000,\n",
      "num_workers=10,\n",
      "output_dir='../models',\n",
      "percent=0.99,\n",
      "qkv_num=8,\n",
      "savename='test',\n",
      "seed=0,\n",
      "softmax_num=8,\n",
      "softmax_p=0.0125,\n",
      "test_T=8,\n",
      "test_mode='ann')\n",
      "Number of the class = 1000\n",
      " patchembed dropout modulelist identity layernormwithforcefp32 linear \n",
      "patch_embed conv2d identity \n",
      "blocks block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block block \n",
      "0 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "1 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "2 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "3 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "4 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "5 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "6 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "7 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "8 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "9 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "10 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "11 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "12 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "13 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "14 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "15 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "16 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "17 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "18 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "19 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "20 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "21 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "22 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "23 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "24 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "25 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "26 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "27 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "28 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "29 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "30 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "31 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "32 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "33 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "34 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "35 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "36 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "37 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "38 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "39 layernormwithforcefp32 attention identity layernormwithforcefp32 mlp \n",
      "attn linear dropout linear dropout myat myat softmax mytestplace mytestplace mytestplace mytestplace mytestplace mytestplace \n",
      "mlp mytestplace linear gelu mytestplace linear dropout \n",
      "Load ckpt from ../models/original_model/eva_giant_patch14_336.pth\n",
      "Load state_dict by model_key = model\n",
      "Weights of VisionTransformer not initialized from pretrained model: ['blocks.0.attn.qkv.bias', 'blocks.1.attn.qkv.bias', 'blocks.2.attn.qkv.bias', 'blocks.3.attn.qkv.bias', 'blocks.4.attn.qkv.bias', 'blocks.5.attn.qkv.bias', 'blocks.6.attn.qkv.bias', 'blocks.7.attn.qkv.bias', 'blocks.8.attn.qkv.bias', 'blocks.9.attn.qkv.bias', 'blocks.10.attn.qkv.bias', 'blocks.11.attn.qkv.bias', 'blocks.12.attn.qkv.bias', 'blocks.13.attn.qkv.bias', 'blocks.14.attn.qkv.bias', 'blocks.15.attn.qkv.bias', 'blocks.16.attn.qkv.bias', 'blocks.17.attn.qkv.bias', 'blocks.18.attn.qkv.bias', 'blocks.19.attn.qkv.bias', 'blocks.20.attn.qkv.bias', 'blocks.21.attn.qkv.bias', 'blocks.22.attn.qkv.bias', 'blocks.23.attn.qkv.bias', 'blocks.24.attn.qkv.bias', 'blocks.25.attn.qkv.bias', 'blocks.26.attn.qkv.bias', 'blocks.27.attn.qkv.bias', 'blocks.28.attn.qkv.bias', 'blocks.29.attn.qkv.bias', 'blocks.30.attn.qkv.bias', 'blocks.31.attn.qkv.bias', 'blocks.32.attn.qkv.bias', 'blocks.33.attn.qkv.bias', 'blocks.34.attn.qkv.bias', 'blocks.35.attn.qkv.bias', 'blocks.36.attn.qkv.bias', 'blocks.37.attn.qkv.bias', 'blocks.38.attn.qkv.bias', 'blocks.39.attn.qkv.bias']\n",
      "number of params: 1013174632\n",
      "Batch size = 128\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "1 : 97.65625 99.21875\n",
      "* Acc@1 97.656 Acc@5 99.219 loss 1.424\n",
      "Test ANN :  [  0/391]  eta: 0:52:43  loss: 1.4243 (1.4243)  acc1: 97.6562 (97.6562)  acc5: 99.2188 (99.2188)  time: 8.0910  data: 2.8387  max mem: 13761\n",
      "2 : 95.3125 100.0\n",
      "* Acc@1 96.484 Acc@5 99.609 loss 1.465\n",
      "3 : 94.53125 99.21875\n",
      "* Acc@1 95.833 Acc@5 99.479 loss 1.486\n",
      "4 : 92.1875 99.21875\n",
      "* Acc@1 94.922 Acc@5 99.414 loss 1.495\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"run_class_finetuning.py\", line 101, in <module>\n",
      "    main(opts)\n",
      "  File \"run_class_finetuning.py\", line 88, in main\n",
      "    test_stats = evaluate(data_loader_val, model, device,args=args,model_without_ddp=model_without_ddp)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/root/autodl-tmp/Transformer-to-SNN-ECMT/engine_for_finetuning.py\", line 23, in evaluate\n",
      "    output = model(images)[0]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/autodl-tmp/Transformer-to-SNN-ECMT/model_eva.py\", line 838, in forward\n",
      "    x1 = self.forward_features(x1, return_patch_tokens=return_patch_tokens,T = i)\n",
      "  File \"/root/autodl-tmp/Transformer-to-SNN-ECMT/model_eva.py\", line 816, in forward_features\n",
      "    x = checkpoint.checkpoint(blk, x, rel_pos_bias,None,T)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
      "    return CheckpointFunction.apply(function, preserve, *args)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
      "    outputs = run_function(*args)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/autodl-tmp/Transformer-to-SNN-ECMT/model_eva.py\", line 402, in forward\n",
      "    x = x + self.drop_path(self.attn(self.norm1(x)))\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/autodl-tmp/Transformer-to-SNN-ECMT/model_eva.py\", line 318, in forward\n",
      "    x = self.proj(x)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python run_class_finetuning.py --eval_data_path ../data/val --nb_classes 1000 --data_set image_folder --model eva_g_patch14 --model_path ../models/original_model/eva_giant_patch14_336.pth --input_size 336 --batch_size 128 --test_mode ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d42e13-93d7-4d6d-8a51-370d4b55067a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e4e3b-fb8c-4fcd-8409-f00b1f58f000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
